{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10955452,"sourceType":"datasetVersion","datasetId":6815388}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom geopy.distance import geodesic\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the dataset\n# Assuming the dataset is in a CSV file named 'transactions.csv'\n# If your data is in a different format, adjust accordingly\ndf = pd.read_csv('/kaggle/input/transactions/transactions.csv')\n\n# Convert date columns to datetime format\ndate_columns = ['transactionDateTime', 'currentExpDate', 'accountOpenDate', 'dateOfLastAddressChange']\nfor col in date_columns:\n    df[col] = pd.to_datetime(df[col], errors='coerce')","metadata":{"id":"NM4Tp78aRgRv","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:40:10.651749Z","iopub.execute_input":"2025-03-15T11:40:10.652100Z","iopub.status.idle":"2025-03-15T11:40:20.818106Z","shell.execute_reply.started":"2025-03-15T11:40:10.652033Z","shell.execute_reply":"2025-03-15T11:40:20.817106Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df=df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:40:20.819216Z","iopub.execute_input":"2025-03-15T11:40:20.819575Z","iopub.status.idle":"2025-03-15T11:40:20.824585Z","shell.execute_reply.started":"2025-03-15T11:40:20.819540Z","shell.execute_reply":"2025-03-15T11:40:20.823493Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------- Feature Engineering --------------\n\n# 1. Velocity of Transactions for Each User\nprint(\"Engineering transaction velocity features...\")\n# Sort transactions by customer and datetime\ndf = df.sort_values(['customerId', 'transactionDateTime'])\n\n# Calculate time difference between consecutive transactions per customer\ndf['prevTransactionTime'] = df.groupby('customerId')['transactionDateTime'].shift(1)\ndf['timeDelta'] = (df['transactionDateTime'] - df['prevTransactionTime']).dt.total_seconds() / 3600  # in hours\n\n# Count transactions in the last 24 hours, 7 days\ndef count_transactions_in_timeframe(group, hours):\n    result = []\n    for i, row in group.iterrows():\n        current_time = row['transactionDateTime']\n        timeframe_start = current_time - pd.Timedelta(hours=hours)\n        count = len(group[(group['transactionDateTime'] > timeframe_start) &\n                          (group['transactionDateTime'] < current_time)])\n        result.append(count)\n    return result\n\n# Apply counting functions to each customer group\ncustomer_groups = df.groupby('customerId')\ndf['txn_count_24h'] = customer_groups.apply(lambda x: count_transactions_in_timeframe(x, 24)).explode().values\ndf['txn_count_7d'] = customer_groups.apply(lambda x: count_transactions_in_timeframe(x, 168)).explode().values  # 7*24=168","metadata":{"id":"qq4yfw1CSdA_","outputId":"711e6875-9879-4c72-9091-628d467f34da","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:40:20.825760Z","iopub.execute_input":"2025-03-15T11:40:20.826176Z","iopub.status.idle":"2025-03-15T11:43:35.954453Z","shell.execute_reply.started":"2025-03-15T11:40:20.826140Z","shell.execute_reply":"2025-03-15T11:43:35.953369Z"}},"outputs":[{"name":"stdout","text":"Engineering transaction velocity features...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 2. Unusual Spending Spikes\nprint(\"Engineering spending pattern features...\")\n# Calculate average transaction amount per customer\ncustomer_avg_amount = df.groupby('customerId')['transactionAmount'].transform('mean')\ncustomer_std_amount = df.groupby('customerId')['transactionAmount'].transform('std')\n\n# Calculate z-score of transaction amount\ndf['amount_zscore'] = (df['transactionAmount'] - customer_avg_amount) / customer_std_amount.replace(0, 1)\n\n# Calculate ratio of current transaction to average\ndf['amount_to_avg_ratio'] = df['transactionAmount'] / customer_avg_amount.replace(0, 1)\n\n# Calculate cumulative amount spent in last 24 hours\ndef sum_amount_in_timeframe(group, hours):\n    result = []\n    for i, row in group.iterrows():\n        current_time = row['transactionDateTime']\n        timeframe_start = current_time - pd.Timedelta(hours=hours)\n        amount_sum = group[(group['transactionDateTime'] > timeframe_start) &\n                           (group['transactionDateTime'] < current_time)]['transactionAmount'].sum()\n        result.append(amount_sum)\n    return result\n\ndf['amount_24h'] = customer_groups.apply(lambda x: sum_amount_in_timeframe(x, 24)).explode().values","metadata":{"id":"9G3jmls2RqD0","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:43:35.955745Z","iopub.execute_input":"2025-03-15T11:43:35.956173Z","iopub.status.idle":"2025-03-15T11:45:26.121698Z","shell.execute_reply.started":"2025-03-15T11:43:35.956133Z","shell.execute_reply":"2025-03-15T11:45:26.120615Z"}},"outputs":[{"name":"stdout","text":"Engineering spending pattern features...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import requests\nimport time\nfrom urllib.parse import quote\n\ndef geocode_merchant(merchant_name, merchant_city, merchant_state, merchant_zip):\n    \"\"\"\n    Uses Nominatim API to geocode a merchant location based on provided details.\n    \n    Args:\n        merchant_name (str): Name of the merchant/business\n        merchant_city (str): City where the merchant is located\n        merchant_state (str): State where the merchant is located\n        merchant_zip (str): ZIP/Postal code of the merchant\n        country (str, optional): Country of the merchant. Defaults to \"USA\".\n        \n    Returns:\n        dict: Dictionary containing latitude, longitude, and display name if found\n              or None if no results were found\n    \"\"\"\n    # Nominatim requires a valid user agent\n    headers = {\n        'User-Agent': 'MerchantGeocoder/1.0 (your-email@example.com)'\n    }\n        # Handle missing da\n    \n    # Format the search query\n    query_parts = []\n    if merchant_name:\n        query_parts.append(str(merchant_name))\n    if merchant_city:\n        query_parts.append(str(merchant_city))\n    if merchant_state:\n        query_parts.append(str(merchant_state))\n    if merchant_zip:\n        query_parts.append(str(merchant_zip))\n    \n    query = \", \".join(query_parts)\n    \n    # URL encode the query\n    encoded_query = quote(query)\n    \n    # Nominatim API endpoint\n    url = f\"https://nominatim.openstreetmap.org/search?q={encoded_query}&format=json&limit=1\"\n    \n    try:\n        # Make the request\n        response = requests.get(url, headers=headers)\n        \n        # Respect Nominatim's usage policy (max 1 request per second)\n        time.sleep(1)\n        \n        if response.status_code == 200:\n            results = response.json()\n            if results:\n                result = results[0]\n                print(result['display_name'])\n                return {\n                    'latitude': float(result['lat']),\n                    'longitude': float(result['lon']),\n                }\n            else:\n                return None\n        else:\n            print(f\"Error: Received status code {response.status_code}\")\n            return None\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:45:26.122699Z","iopub.execute_input":"2025-03-15T11:45:26.122997Z","iopub.status.idle":"2025-03-15T11:45:26.132027Z","shell.execute_reply.started":"2025-03-15T11:45:26.122974Z","shell.execute_reply":"2025-03-15T11:45:26.130705Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 3. Geolocation Analysis for Each User\nprint(\"Engineering geolocation features...\")\n# Create a simple mapping for demonstration\n# In a real scenario, you would use external APIs or databases to get coordinates\n# This is a simplified mapping for demonstration purposes\n\n# Create a dictionary to store merchant location data\n# Format: {merchantName_merchantCity_merchantState: (lat, lon)}\nmerchant_locations = {}\n\n# Function to get merchant location (simulated)\ncounter=0\ndef get_merchant_location(row):\n    merchant_key = f\"{row['merchantName']}_{row['merchantCity']}_{row['merchantState']}_{row['merchantZip']}\"\n    if merchant_key not in merchant_locations:\n        # In real application, use geocoding API to get actual coordinates\n        # Here we simulate with random but consistent coordinates#longitude range approx\n        dicts=geocode_merchant(row['merchantName'],row['merchantCity'],row['merchantState'],row['merchantZip'])\n        if dicts:\n            lat,lon = dicts['latitude'], dicts['longitude']\n        else:\n            lat,lon=None, None\n        merchant_locations[merchant_key] = (lat, lon)\n\n    return merchant_locations[merchant_key]\n\n# Apply the function to create lat/lon columns\ndf['merchant_loc'] = df.apply(get_merchant_location, axis=1)\ndf['merchant_lat'] = df['merchant_loc'].apply(lambda x: x[0])\ndf['merchant_lon'] = df['merchant_loc'].apply(lambda x: x[1])\n","metadata":{"id":"k_ow_5sJRk2S","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:45:26.134746Z","iopub.execute_input":"2025-03-15T11:45:26.135092Z","execution_failed":"2025-03-15T12:29:21.362Z"}},"outputs":[{"name":"stdout","text":"Engineering geolocation features...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Fill previous valid latitude and longitude recursively using ffill()\ndf['prev_lat'] = df.groupby('customerId')['merchant_lat'].ffill().shift(1)\ndf['prev_lon'] = df.groupby('customerId')['merchant_lon'].ffill().shift(1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-15T12:29:21.367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate distance in kilometers\ndef calculate_distance(row):\n    if pd.isna(row['prev_lat']) or pd.isna(row['prev_lon']) or pd.isna(row['merchant_lat']) or pd.isna(row['merchant_lon']):\n        return 0\n    return geodesic((row['prev_lat'], row['prev_lon']), (row['merchant_lat'], row['merchant_lon'])).kilometers\n\ndf['distance_from_prev_txn'] = df.apply(calculate_distance, axis=1)\n\n# Calculate speed (km/h) - distance divided by time difference\ndf['speed_kmph'] = np.where(df['timeDelta'] > 0, df['distance_from_prev_txn'] / df['timeDelta'], 0)\n\n# Calculate if transaction is in a different country from previous\ndf['prev_country'] = df.groupby('customerId')['merchantCountryCode'].shift(1)\ndf['different_country'] = (df['merchantCountryCode'] != df['prev_country']).astype(int)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-15T12:29:21.370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Additional Features\nprint(\"Engineering additional features...\")\n# Binary flags\ndf['cvv_match'] = (df['cardCVV'] == df['enteredCVV']).astype(int)\ndf['exp_date_match'] = df['expirationDateKeyInMatch'].astype(int)\ndf['is_foreign_transaction'] = (df['acqCountry'] != df['merchantCountryCode']).astype(int)\n\n# Calculate the ratio of transaction amount to credit limit\ndf['amount_to_limit_ratio'] = df['transactionAmount'] / df['creditLimit'].replace(0, 1)\n\n# Calculate the ratio of transaction amount to available money\ndf['amount_to_available_ratio'] = df['transactionAmount'] / df['availableMoney'].replace(0, 1)\n\n# Calculate days since account opening\ndf['account_age_days'] = (df['transactionDateTime'] - df['accountOpenDate']).dt.days\n\n# Calculate days since last address change\ndf['days_since_address_change'] = (df['transactionDateTime'] - df['dateOfLastAddressChange']).dt.days\n\ndf['isOnline'] = np.where(df['merchantCategoryCode'] == 'online_retail', 1, 0)\n# One-hot encode categorical variables\ncategorical_cols = ['posEntryMode', 'posConditionCode', 'merchantCategoryCode', 'transactionType']\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)","metadata":{"id":"z8dwKhLySFlS","trusted":true,"execution":{"execution_failed":"2025-03-15T12:29:21.370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# -------------- Data Preparation --------------\nprint(\"Preparing data for modeling...\")\n# Select features for the model\nfeature_cols = [\n    # Transaction velocity features\n    'txn_count_24h', 'txn_count_7d', 'timeDelta',\n\n    # Spending pattern features\n    'amount_zscore', 'amount_to_avg_ratio', 'amount_24h',\n\n    # Geolocation features\n    'distance_from_prev_txn', 'speed_kmph', 'different_country', 'isOnline',\n\n    # Additional features\n    'cvv_match', 'exp_date_match', 'is_foreign_transaction',\n    'amount_to_limit_ratio', 'amount_to_available_ratio',\n    'account_age_days', 'days_since_address_change'\n]\n\n# Add one-hot encoded columns\nfor col in df_encoded.columns:\n    if col.startswith(tuple(categorical_cols)):\n        feature_cols.append(col)\n\n# Remove rows with NaN values in feature columns\ndf_clean = df_encoded.dropna(subset=feature_cols)\n\n# Convert target variable to binary (0 for normal, 1 for fraud)\ny = df_clean['isFraud'].astype(int)\n\n# Select features\nX = df_clean[feature_cols]\n\n# Normalize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n\n","metadata":{"id":"S9GJvUduR6aL","trusted":true,"execution":{"execution_failed":"2025-03-15T12:29:21.371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"isof_params = {\n    'n_estimators': 400,           # Number of trees\n    'max_samples': 'auto',            # Similar to subsample, fraction of dataset used for each tree\n    'contamination': 0.015,       # Adjust based on expected proportion of anomalies\n    'max_features': 0.7,           # Similar to colsample_bytree, fraction of features used per tree\n    'random_state': 42,            # Equivalent to seed\n    'n_jobs': -1                    # Parallel processing\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------- Model Training --------------\nprint(\"Training Isolation Forest model...\")\n# For Isolation Forest, we'll train it on normal transactions only (non-fraud)\nX_train_normal = X_train[y_train == 0]\n\n# Initialize and train the model\n# Contamination is set to a small value since fraud is rare\nmodel = IsolationForest(**isof_params)\nmodel.fit(X_train_normal)\n\n# Predict anomalies\n# Isolation Forest returns -1 for anomalies and 1 for normal samples\n# We convert to 0 for normal and 1 for fraud to match our target\ny_pred_train = (model.predict(X_train) == -1).astype(int)\ny_pred_test = (model.predict(X_test) == -1).astype(int)\n\n# Calculate anomaly scores\nanomaly_scores_test = model.decision_function(X_test)\n# Convert scores to probabilities (lower score = higher probability of fraud)\n# We invert and scale the scores to be between 0 and 1\nprob_fraud = 1 - (anomaly_scores_test - min(anomaly_scores_test)) / (max(anomaly_scores_test) - min(anomaly_scores_test))","metadata":{"id":"ifSykLxLSBXO","trusted":true,"execution":{"execution_failed":"2025-03-15T12:29:21.371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------- Model Evaluation --------------\nprint(\"Evaluating model performance...\")\n# Calculate precision, recall, F1 score\nprecision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average='binary')\n\n# Print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_test))\n\n# Print confusion matrix\ncm = confusion_matrix(y_test, y_pred_test)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\n# Calculate additional metrics for fraud detection\ntn, fp, fn, tp = cm.ravel()\nfraud_detection_rate = tp / (tp + fn)\nfalse_positive_rate = fp / (fp + tn)\n\nprint(f\"\\nFraud Detection Rate (Recall): {fraud_detection_rate:.4f}\")\nprint(f\"False Positive Rate: {false_positive_rate:.4f}\")\nprint(f\"Precision (proportion of flagged transactions that are actually fraud): {precision:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# -------------- Visualization --------------\n# 1. Confusion Matrix Heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.close()\n\n# 2. Precision-Recall Curve\nfrom sklearn.metrics import precision_recall_curve\nprecision_curve, recall_curve, thresholds = precision_recall_curve(y_test, prob_fraud)\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall_curve, precision_curve, marker='.', label=f'Isolation Forest (AUC={precision * recall:.3f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(True)\nplt.savefig('precision_recall_curve.png')\nplt.close()\n\n# 3. Feature Importance\n# Isolation Forest doesn't provide feature importance directly, but we can\n# compute it based on the mean path length decrease for each feature\ndef calculate_feature_importance(model, X):\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    feature_importances = np.zeros(n_features)\n\n    for i in range(n_features):\n        X_permuted = X.copy()\n        np.random.shuffle(X_permuted[:, i])\n\n        # Get original and permuted scores\n        original_scores = model.score_samples(X)\n        permuted_scores = model.score_samples(X_permuted)\n\n        # Calculate importance as the mean decrease in score when permuting the feature\n        feature_importances[i] = np.mean(original_scores) - np.mean(permuted_scores)\n\n    return feature_importances\n\n# Calculate feature importance\nfeature_importance = calculate_feature_importance(model, X_test)\n\n# Sort features by importance\nsorted_idx = np.argsort(feature_importance)\nfeature_names = np.array(feature_cols)\n\nplt.figure(figsize=(10, 8))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\nplt.yticks(range(len(sorted_idx)), feature_names[sorted_idx])\nplt.xlabel('Feature Importance')\nplt.title('Feature Importance for Fraud Detection')\nplt.tight_layout()\nplt.savefig('feature_importance.png')\nplt.close()","metadata":{"id":"pa7puxjSR-94","trusted":true,"execution":{"execution_failed":"2025-03-15T12:29:21.372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------- Save the model --------------\nfrom joblib import dump\ndump(model, 'isolation_forest_fraud_model.joblib')\ndump(scaler, 'feature_scaler.joblib')\n\nprint(\"\\nModel training and evaluation complete.\")\nprint(\"Model saved as 'isolation_forest_fraud_model.joblib'\")\nprint(\"Scaler saved as 'feature_scaler.joblib'\")\n\n# -------------- Example Function for Prediction --------------\ndef predict_fraud(transaction_data, model, scaler, feature_cols):\n    \"\"\"\n    Predict if a transaction is fraudulent\n\n    Parameters:\n    transaction_data: DataFrame row with transaction information\n    model: Trained Isolation Forest model\n    scaler: Fitted StandardScaler\n    feature_cols: List of feature column names\n\n    Returns:\n    is_fraud: Boolean indicating fraud prediction\n    fraud_probability: Estimated probability of fraud\n    \"\"\"\n    # Extract features\n    features = transaction_data[feature_cols].values.reshape(1, -1)\n\n    # Scale features\n    scaled_features = scaler.transform(features)\n\n    # Get anomaly score\n    anomaly_score = model.decision_function(scaled_features)[0]\n\n    # Convert score to probability (lower score = higher probability of fraud)\n    # This is a simple conversion for demonstration purposes\n    fraud_probability = 1 - (anomaly_score + 0.5)  # Adjust range to [0,1]\n    fraud_probability = max(0, min(1, fraud_probability))  # Clip to [0,1]\n\n    # Predict class\n    is_fraud = model.predict(scaled_features)[0] == -1","metadata":{"id":"bBowoqaeSQCR","trusted":true,"execution":{"execution_failed":"2025-03-15T12:29:21.373Z"}},"outputs":[],"execution_count":null}]}