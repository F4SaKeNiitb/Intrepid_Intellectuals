{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10955452,"sourceType":"datasetVersion","datasetId":6815388},{"sourceId":10970341,"sourceType":"datasetVersion","datasetId":6825981},{"sourceId":11041173,"sourceType":"datasetVersion","datasetId":6877526}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CATCHM pipeline [demo]","metadata":{"_uuid":"93f298fe-fe00-4078-95f7-c6f9f69a24f4","_cell_guid":"30acb7ce-0df3-453d-a44a-6df3b06282b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This notebook is in many ways identical to the *CATCHM_demo.ipynb* notebook.\nHowever, in this notebook CATCHM is implemented as a [ScikitLearn compatible pipeline object](https://scikit-learn.org/stable/modules/compose.html). \nThis allows you to experiment with different classifiers and replace the default [XGBoost model](https://xgboost.readthedocs.io/en/stable/python/index.html).","metadata":{"_uuid":"98f60aed-1fe9-4ca5-b77f-5b21bf7b35a6","_cell_guid":"d2363334-556d-4188-8ec6-7fe7abdc082d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install --upgrade numpy pandas nodevectors xgboost fucc optuna scikit-learn cupy-cuda12x numba scipy==1.11.4 networkx==2.5.1","metadata":{"_uuid":"178aba82-fdcf-41e5-a0c8-cd36de801aa1","_cell_guid":"df2a6484-2d0c-4753-8fa9-9d194fdeb99e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:36:58.732978Z","iopub.execute_input":"2025-03-16T15:36:58.733166Z","iopub.status.idle":"2025-03-16T15:37:47.967068Z","shell.execute_reply.started":"2025-03-16T15:36:58.733147Z","shell.execute_reply":"2025-03-16T15:37:47.966254Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nCollecting numpy\n  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nCollecting nodevectors\n  Downloading nodevectors-0.1.23.tar.gz (15 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\nCollecting xgboost\n  Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nCollecting fucc\n  Downloading fucc-0.0.8-py3-none-any.whl.metadata (650 bytes)\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (12.2.0)\nCollecting cupy-cuda12x\n  Downloading cupy_cuda12x-13.4.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.6 kB)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\nCollecting numba\n  Downloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nCollecting scipy==1.11.4\n  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting networkx==2.5.1\n  Downloading networkx-2.5.1-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from networkx==2.5.1) (4.4.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nCollecting csrgraph (from nodevectors)\n  Downloading csrgraph-0.1.28.tar.gz (14 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from nodevectors) (4.3.3)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fucc) (4.67.1)\nRequirement already satisfied: scikit-plot in /usr/local/lib/python3.10/dist-packages (from fucc) (0.3.7)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fucc) (3.7.5)\nCollecting dateparser (from fucc)\n  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (0.8.2)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba)\n  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.10/dist-packages (from dateparser->fucc) (2024.11.6)\nRequirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.10/dist-packages (from dateparser->fucc) (5.2)\nRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->nodevectors) (7.0.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (3.2.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->nodevectors) (1.17.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fucc-0.0.8-py3-none-any.whl (10 kB)\nDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading cupy_cuda12x-13.4.0-cp310-cp310-manylinux2014_x86_64.whl (104.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: nodevectors, csrgraph\n  Building wheel for nodevectors (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for nodevectors: filename=nodevectors-0.1.23-py3-none-any.whl size=17931 sha256=e610b38cb08e21f8eeff06d3a8757b13327e9dc78a5da9b010d74198b4d0902a\n  Stored in directory: /root/.cache/pip/wheels/b5/4c/91/bf05a408dda66b136034cf5c91e4a40381fc01aa0f5ecd8f89\n  Building wheel for csrgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for csrgraph: filename=csrgraph-0.1.28-py3-none-any.whl size=17613 sha256=5f6dce8a7728af716cd0e4efdbbcda21294dbd6c9fa285c58658e6c534f29fc0\n  Stored in directory: /root/.cache/pip/wheels/0f/b5/63/fd61f029bb51e69d1d3ab578bc3361159f52bd99e46bd8d5c3\nSuccessfully built nodevectors csrgraph\nInstalling collected packages: networkx, llvmlite, dateparser, scipy, scikit-learn, numba, csrgraph, xgboost, nodevectors, fucc, cupy-cuda12x\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.4.2\n    Uninstalling networkx-3.4.2:\n      Successfully uninstalled networkx-3.4.2\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.13.1\n    Uninstalling scipy-1.13.1:\n      Successfully uninstalled scipy-1.13.1\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.0.3\n    Uninstalling xgboost-2.0.3:\n      Successfully uninstalled xgboost-2.0.3\n  Attempting uninstall: cupy-cuda12x\n    Found existing installation: cupy-cuda12x 12.2.0\n    Uninstalling cupy-cuda12x-12.2.0:\n      Successfully uninstalled cupy-cuda12x-12.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\ncudf-cu12 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ncuml-cu12 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\nnx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.5.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nscikit-image 0.25.0 requires networkx>=3.0, but you have networkx 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed csrgraph-0.1.28 cupy-cuda12x-13.4.0 dateparser-1.2.1 fucc-0.0.8 llvmlite-0.44.0 networkx-2.5.1 nodevectors-0.1.23 numba-0.61.0 scikit-learn-1.6.1 scipy-1.11.4 xgboost-3.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from networkx.readwrite import edgelist\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\nimport torch\nimport networkx as nx\nfrom nodevectors import Node2Vec\nimport pandas as pd\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:37:47.967970Z","iopub.execute_input":"2025-03-16T15:37:47.968188Z","iopub.status.idle":"2025-03-16T15:38:14.780972Z","shell.execute_reply.started":"2025-03-16T15:37:47.968167Z","shell.execute_reply":"2025-03-16T15:38:14.780206Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def inductive_pooling(edgelist, embeddings, G, workers, gamma=1000, dict_node=None, \n                     average_embedding=True, use_gpu=False, device=None):\n    \"\"\"\n    GPU-accelerated inductive pooling implementation\n    \"\"\"\n    # Convert edgelist to array\n    edgearray = np.array([[str(id), v[0], v[1]] for id, v in enumerate(edgelist)])\n    \n    # Calculate average embedding\n    if average_embedding:\n        if use_gpu and device and device.type == 'cuda':\n            # Calculate on GPU\n            emb_tensor = torch.tensor(embeddings, device=device)\n            avg_emb = emb_tensor.mean(dim=0).cpu().numpy()\n        else:\n            # Calculate on CPU\n            avg_emb = embeddings.mean(axis=0)\n    else:\n        avg_emb = None\n    \n    # Split processing based on GPU availability\n    if use_gpu and device and device.type == 'cuda' and len(edgearray) > 1000:\n        # For large datasets on GPU, we'll process in batches\n        if workers > 1:\n            print(\"Note: Using GPU with multiple workers. This may not be optimal for all systems.\")\n        \n        result_list = []\n        split_arrays = np.array_split(edgearray, workers)\n        \n        for batch in tqdm(split_arrays, total=len(split_arrays)):\n            result = gpu_inductive_pooling_batch(batch, embeddings, G, average_embedding=avg_emb, device=device)\n            result_list.append(result)\n    else:\n        # Use CPU multiprocessing for smaller datasets or if GPU is not available\n        result_list = []\n        with Pool(workers) as p:\n            for result in tqdm(p.imap(partial(inductive_pooling_chunk, \n                                             embeddings=embeddings, \n                                             G=G, \n                                             average_embedding=avg_emb), \n                                     np.array_split(edgearray, workers)), \n                              total=len(np.array_split(edgearray, workers))):\n                result_list.append(result)\n    \n    # Combine results\n    new_embeddings = np.zeros((len(edgelist), embeddings.shape[1]))\n    for result_dict in result_list:\n        for id, emb in result_dict.items():\n            new_embeddings[int(id), :] = emb\n    \n    return new_embeddings\n\ndef gpu_inductive_pooling_batch(edgearray, embeddings, G, gamma=1000, average_embedding=None, device=None):\n    \"\"\"\n    GPU-accelerated version of inductive pooling for batch processing\n    \"\"\"\n    # Convert embeddings to GPU tensor if not already\n    if not torch.is_tensor(embeddings):\n        embeddings_tensor = torch.tensor(embeddings, device=device)\n    else:\n        embeddings_tensor = embeddings\n        \n    # Create container for new embeddings\n    new_embeddings = dict()\n    \n    for row in edgearray:\n        transfer, sender, receiver = row\n        mutual = False\n        \n        if G.has_node(sender) and G.has_node(receiver):\n            mutual_neighbors = list(set(G.neighbors(sender)).intersection(set(G.neighbors(receiver))))\n            # Convert string ids to numerical ids \n            mutual_neighbors = list(map(int, mutual_neighbors))\n            # Sort numerical ids\n            mutual_neighbors.sort()\n            \n            if len(mutual_neighbors) > 0:\n                mutual = True\n                # Take most recent mutual neighbor\n                most_recent_mutual_neighbor = mutual_neighbors[-1]\n                # Get embedding from GPU tensor\n                most_recent_embedding = embeddings_tensor[most_recent_mutual_neighbor].cpu().numpy()\n                new_embeddings[transfer] = most_recent_embedding\n                \n        if G.has_node(sender) and (not mutual):\n            sender_neighbors = list(map(int, G.neighbors(sender)))\n            pooled_embedding = get_pooled_embedding_gpu(sender_neighbors, embeddings_tensor, gamma, device)\n            new_embeddings[transfer] = pooled_embedding\n            \n        elif G.has_node(receiver) and (not mutual):\n            receiver_neighbors = list(map(int, G.neighbors(receiver)))\n            pooled_embedding = get_pooled_embedding_gpu(receiver_neighbors, embeddings_tensor, gamma, device)\n            new_embeddings[transfer] = pooled_embedding\n            \n        elif not mutual:\n            # Use average embedding as fallback\n            if torch.is_tensor(average_embedding):\n                new_embeddings[transfer] = average_embedding.cpu().numpy()\n            else:\n                new_embeddings[transfer] = average_embedding\n                \n    return new_embeddings\n\ndef get_pooled_embedding_gpu(neighbors, embeddings_tensor, gamma, device):\n    \"\"\"\n    GPU-accelerated version of pooled embedding calculation\n    \"\"\"\n    # Extract embeddings for neighbors\n    if len(neighbors) == 0:\n        # Return zeros if no neighbors\n        return torch.zeros(embeddings_tensor.shape[1], device=device).cpu().numpy()\n    \n    # Get indices for neighbors\n    indices = torch.tensor(neighbors, device=device)\n    \n    # Use only the most recent gamma neighbors\n    start_idx = max(0, len(neighbors) - gamma)\n    indices = indices[start_idx:]\n    \n    # Get embeddings for these neighbors\n    neighbor_embeddings = torch.index_select(embeddings_tensor, 0, indices)\n    \n    # Calculate mean embedding\n    pooled_embedding = torch.mean(neighbor_embeddings, dim=0)\n    \n    # Return as numpy array\n    return pooled_embedding.cpu().numpy()\n\ndef inductive_pooling_chunk(edgearray, embeddings, G, gamma=1000, average_embedding=None):\n    \"\"\"\n    CPU version of inductive pooling for a chunk of edges\n    \"\"\"\n    # Create a container for the new embeddings\n    new_embeddings = dict()\n    for row in edgearray:\n        transfer, sender, receiver = row\n        mutual = False    \n        if G.has_node(sender) and G.has_node(receiver):\n            mutual_neighbors = list(set(G.neighbors(sender)).intersection(set(G.neighbors(receiver))))\n            # convert string ids to numerical ids \n            mutual_neighbors = list(map(int, mutual_neighbors))\n            # sort numerical ids\n            mutual_neighbors.sort()\n            \n            if (len(mutual_neighbors) > 0): \n                mutual = True\n                # take most recent mutual neighbor\n                most_recent_mutual_neighbor = mutual_neighbors[-1]\n                # Use dataframe with TX_ID on index (to speed up retrieval of transfer rows)\n                most_recent_embedding_mutual_neighbor = embeddings[most_recent_mutual_neighbor, :]\n                new_embeddings[transfer] = most_recent_embedding_mutual_neighbor\n                \n                        \n        if G.has_node(sender) and (not mutual):\n            sender_neighbors = list(map(int, G.neighbors(sender)))\n            pooled_embedding = get_pooled_embedding(sender_neighbors, embeddings, gamma)\n            \n            new_embeddings[transfer] = pooled_embedding\n            \n        elif G.has_node(receiver) and (not mutual):\n            receiver_neighbors = list(map(int, G.neighbors(receiver)))\n            pooled_embedding = get_pooled_embedding(receiver_neighbors, embeddings, gamma)\n            new_embeddings[transfer] = pooled_embedding\n            \n            \n        elif (not mutual):\n            new_embeddings[transfer] = average_embedding\n                    \n    return new_embeddings\n                            \ndef get_pooled_embedding(neighbors, embeddings, gamma):\n    \"\"\"\n    CPU version of pooled embedding calculation\n    \"\"\"\n    if len(neighbors) == 0:\n        # Return zeros if no neighbors\n        return np.zeros(embeddings.shape[1])\n        \n    embeddings_to_pool = embeddings[neighbors, :]\n    most_recent_embeddings_to_pool = embeddings_to_pool[-min(gamma, embeddings_to_pool.shape[0]):, :]\n    \n    pooled_embedding = most_recent_embeddings_to_pool.mean(axis=0)\n    \n    return pooled_embedding\n\n# Assume EpochLogger implementation is needed\nclass EpochLogger:\n    \"\"\"\n    Callback to log epoch progress\n    \"\"\"\n    def __init__(self):\n        self.epoch = 0\n        \n    def on_epoch_end(self, model):\n        self.epoch += 1\n        print(f\"Completed epoch {self.epoch}\")","metadata":{"_uuid":"217274e4-c3b2-4f1f-a551-d7ce1e57f174","_cell_guid":"b9d77ceb-3d55-4a5e-bcc8-0114d57a1486","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:38:14.782025Z","iopub.execute_input":"2025-03-16T15:38:14.782711Z","iopub.status.idle":"2025-03-16T15:38:14.801569Z","shell.execute_reply.started":"2025-03-16T15:38:14.782686Z","shell.execute_reply":"2025-03-16T15:38:14.800957Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm import tqdm\n\ndef create_network(X_train, y_train, use_gpu=True, batch_size=10000, verbose=True):\n    \"\"\"\n    GPU-accelerated function to create a network structure optimized for fraud detection.\n    \n    Parameters\n    ----------\n    X_train : pandas.DataFrame\n        DataFrame containing transaction features\n    y_train : pandas.Series\n        Series containing fraud labels (1 for fraud, 0 for non-fraud)\n    use_gpu : bool, default=True\n        Whether to use GPU acceleration\n    batch_size : int, default=10000\n        Batch size for GPU operations\n    verbose : bool, default=True\n        Whether to show progress bars\n        \n    Returns\n    -------\n    G : networkx.Graph\n        Graph with nodes representing transactions, customers, merchants, and an artificial fraud node\n    \"\"\"\n    # Check if GPU is available when requested\n    if use_gpu:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        if verbose and device.type == 'cuda':\n            print(f\"Using GPU for network creation: {torch.cuda.get_device_name(0)}\")\n        elif verbose:\n            print(\"GPU requested but not available. Using CPU instead.\")\n    else:\n        device = torch.device('cpu')\n        if verbose:\n            print(\"Using CPU as requested.\")\n    \n    # Create graph\n    G = nx.Graph()\n    \n    # Generate IDs\n    transaction_ids = [f\"txn_{i}\" for i in range(len(X_train))]\n    customer_ids = [f\"cust_{str(cid)}\" for cid in X_train['customerId']]\n    merchant_ids = [f\"merch_{name}_{country}\" for name, country in \n                  zip(X_train['merchantName'], X_train['merchantCountryCode'])]\n    \n    if verbose:\n        print(\"Adding transaction nodes...\")\n    \n    # Add nodes with attributes\n    for i, txn_id in enumerate(tqdm(transaction_ids) if verbose else transaction_ids):\n        # Add transaction node with relevant features\n        G.add_node(txn_id, \n                  type='transaction',\n                  amount_zscore=X_train.iloc[i]['amount_zscore'],\n                  amount_to_avg_ratio=X_train.iloc[i]['amount_to_avg_ratio'],\n                  is_foreign=X_train.iloc[i]['is_foreign_transaction'],\n                  cvv_match=X_train.iloc[i]['cvv_match'],\n                  exp_date_match=X_train.iloc[i]['exp_date_match'])\n    \n    if verbose:\n        print(\"Adding customer and merchant nodes...\")\n    \n    # Add customer and merchant nodes\n    G.add_nodes_from(set(customer_ids), type='customer')\n    G.add_nodes_from(set(merchant_ids), type='merchant')\n    \n    if verbose:\n        print(\"Creating transaction-entity edges...\")\n    \n    # Create edges between transactions and entities\n    for i, txn_id in enumerate(tqdm(transaction_ids) if verbose else transaction_ids):\n        # Connect transaction to customer\n        G.add_edge(txn_id, customer_ids[i], edge_type='customer_transaction')\n        \n        # Connect transaction to merchant\n        G.add_edge(txn_id, merchant_ids[i], edge_type='merchant_transaction')\n    \n    if verbose:\n        print(\"Processing merchant proximity...\")\n    \n    # Extract merchant coordinates\n    merchant_df = pd.DataFrame({\n        'merchant_id': merchant_ids,\n        'lat': X_train['merchant_lat'].values,\n        'lon': X_train['merchant_lon'].values\n    }).drop_duplicates('merchant_id')\n    \n    # Filter out merchants with invalid coordinates\n    valid_merchant_df = merchant_df.dropna()\n    \n    # GPU-accelerated geographical proximity calculation\n    if len(valid_merchant_df) > 0:\n        if use_gpu and device.type == 'cuda':\n            # Use GPU for proximity calculation\n            coords = torch.tensor(valid_merchant_df[['lat', 'lon']].values, device=device, dtype=torch.float32)\n            \n            # Process in batches if dataset is large\n            merchant_edges = []\n            n_merchants = len(valid_merchant_df)\n            \n            for i in range(0, n_merchants, batch_size):\n                end_idx = min(i + batch_size, n_merchants)\n                batch_coords = coords[i:end_idx]\n                \n                # Calculate pairwise distances using GPU\n                # ||a - b||^2 = ||a||^2 + ||b||^2 - 2*a*b\n                a_norm = torch.sum(batch_coords**2, dim=1).view(-1, 1)\n                b_norm = torch.sum(coords**2, dim=1).view(1, -1)\n                dist_matrix = a_norm + b_norm - 2 * torch.mm(batch_coords, coords.t())\n                dist_matrix = torch.sqrt(torch.clamp(dist_matrix, min=0))\n                \n                # Find close merchants\n                close_pairs = torch.nonzero(dist_matrix < 0.01, as_tuple=False)\n                \n                # Add valid pairs to edges list\n                for pair in close_pairs:\n                    idx1, idx2 = pair[0].item() + i, pair[1].item()\n                    if idx1 != idx2:  # Avoid self-loops\n                        dist = dist_matrix[pair[0], pair[1]].item()\n                        m1 = valid_merchant_df.iloc[idx1]['merchant_id']\n                        m2 = valid_merchant_df.iloc[idx2]['merchant_id']\n                        merchant_edges.append((m1, m2, {'edge_type': 'location_proximity', 'weight': 1-dist*100}))\n            \n            # Add all edges to graph\n            G.add_edges_from(merchant_edges)\n        \n        else:\n            # CPU-based implementation using scikit-learn's NearestNeighbors\n            coords = valid_merchant_df[['lat', 'lon']].values\n            \n            # Use NearestNeighbors for efficient proximity search\n            nbrs = NearestNeighbors(radius=0.01, algorithm='ball_tree').fit(coords)\n            distances, indices = nbrs.radius_neighbors(coords)\n            \n            # Create edges for close merchants\n            merchant_edges = []\n            for i, idx_list in enumerate(indices):\n                for j, dist in zip(idx_list, distances[i]):\n                    if i != j:  # Avoid self-loops\n                        m1 = valid_merchant_df.iloc[i]['merchant_id']\n                        m2 = valid_merchant_df.iloc[j]['merchant_id']\n                        merchant_edges.append((m1, m2, {'edge_type': 'location_proximity', 'weight': 1-dist*100}))\n            \n            # Add all edges to graph\n            G.add_edges_from(merchant_edges)\n    \n    if verbose:\n        print(\"Processing sequential transactions...\")\n    \n    # Create a mapping from customer to their transactions\n    customer_txn_map = {}\n    for i, cust_id in enumerate(customer_ids):\n        if cust_id not in customer_txn_map:\n            customer_txn_map[cust_id] = []\n        customer_txn_map[cust_id].append((transaction_ids[i], X_train.iloc[i]['timeDelta']))\n    \n    # Connect transactions from the same customer if they occurred within a short time window\n    sequential_edges = []\n    for cust_id, txns in customer_txn_map.items():\n        if len(txns) > 1:\n            # Sort transactions by time\n            txns.sort(key=lambda x: x[1])\n            \n            # Connect sequential transactions within 24 hours\n            for i in range(len(txns) - 1):\n                for j in range(i + 1, len(txns)):\n                    time_delta = abs(txns[j][1] - txns[i][1])\n                    if time_delta < 86400:  # Within 24 hours (in seconds)\n                        sequential_edges.append((txns[i][0], txns[j][0], {\n                            'edge_type': 'sequential_transactions',\n                            'time_diff': time_delta\n                        }))\n    \n    # Add all sequential edges to graph\n    G.add_edges_from(sequential_edges)\n    \n    if verbose:\n        print(\"Adding fraud node connections...\")\n    \n    # Add artificial fraud node and connect it to all fraudulent transactions\n    fraud_node = \"ARTIFICIAL_FRAUD_NODE\"\n    G.add_node(fraud_node, type='artificial_fraud')\n    \n    # Connect to fraudulent transactions\n    fraud_edges = [(txn_id, fraud_node, {'edge_type': 'is_fraud'}) \n                  for i, txn_id in enumerate(transaction_ids) if y_train.iloc[i] == 1]\n    G.add_edges_from(fraud_edges)\n    \n    if verbose:\n        print(f\"Network creation complete. Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n    \n    return G\n\n# Optional: A utility function to estimate memory requirements\ndef estimate_network_memory(X_train, y_train):\n    \"\"\"\n    Estimates the memory requirements for network creation\n    \"\"\"\n    n_transactions = len(X_train)\n    n_customers = X_train['customerId'].nunique()\n    n_merchants = X_train[['merchantName', 'merchantCountryCode']].drop_duplicates().shape[0]\n    \n    # Estimate nodes (transactions, customers, merchants, fraud node)\n    n_nodes = n_transactions + n_customers + n_merchants + 1\n    \n    # Estimate edges (transaction-customer, transaction-merchant, merchant proximity, sequential, fraud)\n    n_edges_base = n_transactions * 2  # Each transaction connects to a customer and merchant\n    n_fraud_edges = y_train.sum()\n    \n    # Rough estimate of merchant proximity edges (assuming 5% of merchants are close)\n    n_merchant_proximity = int(n_merchants * n_merchants * 0.05)\n    \n    # Rough estimate of sequential transaction edges (assuming 10% of transactions per customer are sequential)\n    avg_txn_per_customer = n_transactions / n_customers\n    n_sequential = int(n_customers * (avg_txn_per_customer * (avg_txn_per_customer - 1) / 2) * 0.1)\n    \n    total_edges = n_edges_base + n_fraud_edges + n_merchant_proximity + n_sequential\n    \n    # Estimate memory (rough approximation)\n    memory_per_node = 100  # bytes\n    memory_per_edge = 60   # bytes\n    \n    estimated_memory = (n_nodes * memory_per_node + total_edges * memory_per_edge) / (1024 * 1024)  # in MB\n    \n    return {\n        'n_nodes': n_nodes,\n        'n_edges': total_edges,\n        'estimated_memory_mb': estimated_memory,\n        'recommended_gpu_memory_gb': max(2, int(estimated_memory / 1024 * 3))  # 3x buffer\n    }","metadata":{"_uuid":"f26d7c72-6eda-48c3-bbc8-c3f648904ea6","_cell_guid":"4282c00a-d7c5-4c9b-ad77-71be760a7929","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:38:14.803305Z","iopub.execute_input":"2025-03-16T15:38:14.803506Z","iopub.status.idle":"2025-03-16T15:38:14.846699Z","shell.execute_reply.started":"2025-03-16T15:38:14.803488Z","shell.execute_reply":"2025-03-16T15:38:14.845898Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from gensim.models.callbacks import CallbackAny2Vec\n\ndef check_edgelist(edgelist):\n\n    if not isinstance(edgelist, list):\n        edgelist = list(edgelist)\n    \n\nclass EpochLogger(CallbackAny2Vec):\n    '''Callback to log information about training'''\n\n    def __init__(self):\n        self.epoch = 0\n\n    def on_epoch_begin(self, model):\n        print(\"Epoch #{} start\".format(self.epoch))\n\n    def on_epoch_end(self, model):\n        print(\"Epoch #{} end\".format(self.epoch))\n        self.epoch += 1","metadata":{"_uuid":"73f50e47-8cdc-42f4-ae9c-2742651113ac","_cell_guid":"a776b68f-cbc7-4c0e-8005-638adbe50d88","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:38:14.848228Z","iopub.execute_input":"2025-03-16T15:38:14.848726Z","iopub.status.idle":"2025-03-16T15:38:14.853476Z","shell.execute_reply.started":"2025-03-16T15:38:14.848704Z","shell.execute_reply":"2025-03-16T15:38:14.852755Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class InductiveDeepwalk(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Implementation of DeepWalk with inductive capabilities for fraud detection.\n    GPU-accelerated version.\n    \n    Parameters\n    ----------\n    dimensions : int\n        Number of dimensions in the embeddings\n    walk_len : int\n        Length of each random walk\n    walk_num : int\n        Number of random walks per node\n    epochs : int, default=5\n        Number of training epochs\n    workers : int, default=1\n        Number of parallel workers\n    window_size : int, default=5\n        Context window size for Word2Vec\n    verbose : int, default=0\n        Verbosity level\n    use_gpu : bool, default=True\n        Whether to use GPU acceleration\n    \"\"\"\n    def __init__(self, dimensions, walk_len, walk_num, epochs=5, workers=1, window_size=5, verbose=0, use_gpu=True):\n        self.dimensions = dimensions\n        self.walk_len = walk_len\n        self.walk_num = walk_num\n        self.epochs = epochs\n        self.workers = workers\n        self.window_size = window_size\n        self.first_fit = True\n        self.verbose = verbose\n        self.use_gpu = use_gpu\n        \n        # Check if GPU is available\n        if self.use_gpu:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            if self.verbose > 0:\n                print(f\"Using device: {self.device}\")\n            if self.device.type == 'cpu' and self.use_gpu:\n                print(\"Warning: GPU requested but not available. Using CPU instead.\")\n        else:\n            self.device = torch.device('cpu')\n            if self.verbose > 0:\n                print(\"Using CPU as requested.\")\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model with X.\n        \n        Parameters\n        ----------\n        X : pandas.DataFrame\n            Training data\n        y : pandas.Series\n            Target values (fraud labels)\n            \n        Returns\n        -------\n        self : object\n            Returns self\n        \"\"\"\n        if self.verbose > 0:\n            print(\"Parsing input into network format.\")\n        \n        # Create network using the updated function\n        self.G = create_network(X, y)\n        \n        # Get transaction nodes\n        transaction_nodes = [n for n, d in self.G.nodes(data=True) if d.get('type') == 'transaction']\n        \n        # Extract transaction IDs as integers for proper indexing\n        self.transaction_ids = [n.split('_')[1] for n in transaction_nodes]\n        \n        callbacks = []\n        if self.verbose > 0:\n            print(\"Running network representation algorithm.\")\n            epochlogger = EpochLogger()\n            callbacks = [epochlogger]\n        \n        # Configure Word2Vec parameters with GPU support if available\n        w2v_params = {\n            'workers': self.workers, \n            'window': self.window_size, \n            'callbacks': callbacks,\n            'compute_loss': True\n        }\n        \n        # Train Node2Vec model\n        g2v = Node2Vec(\n            n_components=self.dimensions,\n            walklen=self.walk_len,\n            epochs=self.walk_num,\n            verbose=self.verbose,\n            w2vparams=w2v_params\n        )\n        g2v.fit(self.G)\n        self.model = g2v.model\n        \n        # Create dictionary of node embeddings\n        self.node_embeddings = {}\n        for node in self.G.nodes():\n            try:\n                self.node_embeddings[node] = self.model.wv[node]\n            except KeyError:\n                # Handle nodes not in vocabulary\n                self.node_embeddings[node] = np.zeros(self.dimensions)\n        \n        # Create array of transaction embeddings for easier access\n        self.embeddings = np.zeros((len(transaction_nodes), self.dimensions))\n        for i, txn_id in enumerate(transaction_nodes):\n            self.embeddings[i] = self.node_embeddings[txn_id]\n            \n        # Convert embeddings to PyTorch tensors for GPU processing\n        if self.use_gpu and self.device.type == 'cuda':\n            self.embeddings_tensor = torch.tensor(self.embeddings, device=self.device)\n        \n        self.is_fitted_ = True\n        self.first_fit = True\n        return self\n    \n    def transform(self, X):\n        \"\"\"\n        Transform X.\n        \n        Parameters\n        ----------\n        X : pandas.DataFrame\n            Test data containing transaction features\n            \n        Returns\n        -------\n        np.ndarray\n            Transaction embeddings\n        \"\"\"\n        check_is_fitted(self, 'is_fitted_')\n        \n        if self.first_fit:\n            if self.verbose > 0:\n                print(\"Retrieving embeddings for training data.\")\n            results = self.embeddings\n            self.first_fit = False\n        else:\n            if self.verbose > 0:\n                print(\"Running inductive pooling extension.\")\n            results = inductive_pooling(X, self.embeddings, self.G, workers=self.workers, \n                                       use_gpu=self.use_gpu, device=self.device)\n        \n        return results","metadata":{"_uuid":"150e5567-445a-468b-afde-102c14bf0cc3","_cell_guid":"1d0cd7f1-a0aa-4e1e-b4a9-7a26381326a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:38:14.854173Z","iopub.execute_input":"2025-03-16T15:38:14.854429Z","iopub.status.idle":"2025-03-16T15:38:14.874999Z","shell.execute_reply.started":"2025-03-16T15:38:14.854405Z","shell.execute_reply":"2025-03-16T15:38:14.874230Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom geopy.distance import geodesic\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"b1fc45ff-b955-4912-8204-62d06f6f7a04","_cell_guid":"5a302ffb-991e-4bb2-b544-0c9b90bffddb","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:38:14.875726Z","iopub.execute_input":"2025-03-16T15:38:14.875955Z","iopub.status.idle":"2025-03-16T15:38:15.400734Z","shell.execute_reply.started":"2025-03-16T15:38:14.875923Z","shell.execute_reply":"2025-03-16T15:38:15.400055Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Parameters\ndimensions = 32\nwalk_len = 80\nwalk_num = 10\nwindow_size = 5\n# the 'workers' parameter is used for multi-processing.\nworkers = 4","metadata":{"_uuid":"84b03b20-4ad1-4e65-bb9c-272027ad3a47","_cell_guid":"60b513e1-41d7-450f-bd5b-485efeaa3230","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:38:15.401407Z","iopub.execute_input":"2025-03-16T15:38:15.401894Z","iopub.status.idle":"2025-03-16T15:38:15.405623Z","shell.execute_reply.started":"2025-03-16T15:38:15.401870Z","shell.execute_reply":"2025-03-16T15:38:15.404674Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Load Data","metadata":{"_uuid":"2ef7befb-daec-43dc-84b1-25aaab5dce7c","_cell_guid":"d5714981-f9ef-4447-9ec6-88390f2f2f40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\n### PATH TO DEMO DATA ###\ndemo_data_path = '/kaggle/input/latlondata/output.csv'\n\ndf = pd.read_csv(demo_data_path)\n\n# Convert date columns to datetime format\ndate_columns = ['transactionDateTime', 'currentExpDate', 'accountOpenDate', 'dateOfLastAddressChange']\nfor col in date_columns:\n    df[col] = pd.to_datetime(df[col], errors='coerce')","metadata":{"_uuid":"de45ead8-d2fd-4a0e-b2b4-333166cb7598","_cell_guid":"8b6a03d4-31b9-45a9-9d75-f940a22cd03c","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T15:38:15.406413Z","iopub.execute_input":"2025-03-16T15:38:15.406697Z","iopub.status.idle":"2025-03-16T15:38:20.181219Z","shell.execute_reply.started":"2025-03-16T15:38:15.406661Z","shell.execute_reply":"2025-03-16T15:38:20.180234Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df.dropna(subset=['merchant_lat'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:38:20.182183Z","iopub.execute_input":"2025-03-16T15:38:20.182457Z","iopub.status.idle":"2025-03-16T15:38:20.276927Z","shell.execute_reply.started":"2025-03-16T15:38:20.182433Z","shell.execute_reply":"2025-03-16T15:38:20.276205Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:38:20.280108Z","iopub.execute_input":"2025-03-16T15:38:20.280352Z","iopub.status.idle":"2025-03-16T15:38:20.921757Z","shell.execute_reply.started":"2025-03-16T15:38:20.280333Z","shell.execute_reply":"2025-03-16T15:38:20.920551Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(303005, 33)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import cudf\nimport cupy as cp\nimport pandas as pd\nfrom cuml.metrics import pairwise_distances\nimport numpy as np\nfrom datetime import datetime\n\n# Convert pandas DataFrame to cuDF DataFrame\nprint(\"Converting DataFrame to GPU...\")\ntry:\n    # If df is already in memory as pandas DataFrame\n    df_gpu = cudf.DataFrame.from_pandas(df)\nexcept NameError:\n    # If you're loading from file directly\n    # df_gpu = cudf.read_csv('your_transaction_data.csv', parse_dates=['transactionDateTime', 'accountOpenDate', 'dateOfLastAddressChange'])\n    print(\"Error: DataFrame 'df' not found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:38:20.923816Z","iopub.execute_input":"2025-03-16T15:38:20.924207Z","iopub.status.idle":"2025-03-16T15:38:28.844250Z","shell.execute_reply.started":"2025-03-16T15:38:20.924167Z","shell.execute_reply":"2025-03-16T15:38:28.843365Z"}},"outputs":[{"name":"stdout","text":"Converting DataFrame to GPU...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# -------------- GPU-Accelerated Feature Engineering --------------\nimport cudf\nimport cupy as cp\nimport pandas as pd\nfrom cuml.metrics import pairwise_distances\nimport numpy as np\nfrom datetime import datetime\n\n\nprint(\"Engineering transaction velocity features...\")\n# Sort transactions by customer and datetime\ndf_gpu = df_gpu.sort_values(['customerId', 'transactionDateTime'])\n\n# Calculate time difference between consecutive transactions per customer\ndf_gpu['prevTransactionTime'] = df_gpu.groupby('customerId')['transactionDateTime'].shift(1)\ndf_gpu['timeDelta'] = (df_gpu['transactionDateTime'] - df_gpu['prevTransactionTime']).dt.total_seconds() / 3600  # in hours\n\n# Count transactions in timeframes using GPU-accelerated approach\ndef count_transactions_in_timeframe_gpu(df, hours):\n    # Create a GPU DataFrame with customer ID, transaction time, and a row number\n    temp_df = cudf.DataFrame()\n    temp_df['customerId'] = df['customerId']\n    temp_df['transactionDateTime'] = df['transactionDateTime']\n    \n    # For each transaction, calculate the time window start\n    timeframe_start = df['transactionDateTime'] - pd.Timedelta(hours=hours)\n    \n    # Use GPU to count transactions in time window for each row\n    result = cudf.Series(cp.zeros(len(df), dtype=int))\n    \n    # Process in chunks to avoid memory issues on GPU\n    chunk_size = 1000000  # Adjust based on your GPU memory\n    \n    for i in range(0, len(df), chunk_size):\n        end_idx = min(i + chunk_size, len(df))\n        chunk_customers = df['customerId'].iloc[i:end_idx]\n        chunk_times = df['transactionDateTime'].iloc[i:end_idx]\n        chunk_starts = timeframe_start.iloc[i:end_idx]\n        \n        # For each row in the chunk\n        for j in range(len(chunk_customers)):\n            # Get customer ID and time window\n            cust_id = chunk_customers.iloc[j]\n            cur_time = chunk_times.iloc[j]\n            start_time = chunk_starts.iloc[j]\n            \n            # Count transactions for this customer in the time window\n            count = len(df[(df['customerId'] == cust_id) & \n                          (df['transactionDateTime'] > start_time) &\n                          (df['transactionDateTime'] < cur_time)])\n            \n            result[i + j] = count\n    \n    return result\n\n# Apply counting functions - note: for large datasets, this is still computationally intensive\n# For production, consider a more optimized windowing approach\nprint(\"Counting transactions in timeframes (this may take some time)...\")\ndf_gpu['txn_count_24h'] = count_transactions_in_timeframe_gpu(df_gpu, 24)\ndf_gpu['txn_count_7d'] = count_transactions_in_timeframe_gpu(df_gpu, 168)  # 7*24=168","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:38:28.845207Z","iopub.execute_input":"2025-03-16T15:38:28.845968Z"}},"outputs":[{"name":"stdout","text":"Engineering transaction velocity features...\nCounting transactions in timeframes (this may take some time)...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 2. Unusual Spending Spikes\nprint(\"Engineering spending pattern features...\")\n# Calculate average transaction amount per customer\ncustomer_avg_amount = df_gpu.groupby('customerId')['transactionAmount'].transform('mean')\ncustomer_std_amount = df_gpu.groupby('customerId')['transactionAmount'].transform('std')\n\n# Calculate z-score of transaction amount\ndf_gpu['amount_zscore'] = (df_gpu['transactionAmount'] - customer_avg_amount) / customer_std_amount.fillna(1)\n# Calculate ratio of current transaction to average\ndf_gpu['amount_to_avg_ratio'] = df_gpu['transactionAmount'] / customer_avg_amount.fillna(1)\n\n# Calculate cumulative amount spent in last 24 hours - similar approach as transaction count\ndef sum_amount_in_timeframe_gpu(df, hours):\n    temp_df = cudf.DataFrame()\n    temp_df['customerId'] = df['customerId']\n    temp_df['transactionDateTime'] = df['transactionDateTime']\n    temp_df['transactionAmount'] = df['transactionAmount']\n    \n    # For each transaction, calculate the time window start\n    timeframe_start = df['transactionDateTime'] - pd.Timedelta(hours=hours)\n    \n    # Use GPU to calculate sum in time window for each row\n    result = cudf.Series(cp.zeros(len(df), dtype=float))\n    \n    # Process in chunks to avoid memory issues on GPU\n    chunk_size = 1000000  # Adjust based on your GPU memory\n    \n    for i in range(0, len(df), chunk_size):\n        end_idx = min(i + chunk_size, len(df))\n        chunk_customers = df['customerId'].iloc[i:end_idx]\n        chunk_times = df['transactionDateTime'].iloc[i:end_idx]\n        chunk_starts = timeframe_start.iloc[i:end_idx]\n        \n        # For each row in the chunk\n        for j in range(len(chunk_customers)):\n            # Get customer ID and time window\n            cust_id = chunk_customers.iloc[j]\n            cur_time = chunk_times.iloc[j]\n            start_time = chunk_starts.iloc[j]\n            \n            # Sum amounts for this customer in the time window\n            amount_sum = df[(df['customerId'] == cust_id) & \n                           (df['transactionDateTime'] > start_time) &\n                           (df['transactionDateTime'] < cur_time)]['transactionAmount'].sum()\n            \n            result[i + j] = amount_sum\n    \n    return result\n\nprint(\"Calculating spending amounts in timeframes...\")\ndf_gpu['amount_24h'] = sum_amount_in_timeframe_gpu(df_gpu, 24)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Engineering geographic features...\")\n# Fill previous valid latitude and longitude recursively using ffill()\ndf_gpu['prev_lat'] = df_gpu.groupby('customerId')['merchant_lat'].ffill().shift(1)\ndf_gpu['prev_lon'] = df_gpu.groupby('customerId')['merchant_lon'].ffill().shift(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def haversine_distance_gpu(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth specified in decimal degrees using GPU acceleration\n    \"\"\"\n    # Convert decimal degrees to radians \n    lat1, lon1, lat2, lon2 = map(cp.radians, [lat1, lon1, lat2, lon2])\n    \n    # Haversine formula\n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = cp.sin(dlat/2)**2 + cp.cos(lat1) * cp.cos(lat2) * cp.sin(dlon/2)**2\n    c = 2 * cp.arcsin(cp.sqrt(a)) \n    r = 6371  # Radius of earth in kilometers\n    return c * r\n\n# Apply GPU-accelerated distance calculation\ndistances = cp.zeros(len(df_gpu))\n\n# Fix: Use .to_cupy() instead of .to_array() for cuDF Series\nmask = (~df_gpu['prev_lat'].isna() & ~df_gpu['prev_lon'].isna() & \n        ~df_gpu['merchant_lat'].isna() & ~df_gpu['merchant_lon'].isna())\n\nif mask.any():\n    # Convert the mask to a cupy array\n    mask_array = mask.to_cupy()\n    valid_indices = cp.where(mask_array)[0]\n    \n    # Convert Series to cupy arrays\n    prev_lat = df_gpu['prev_lat'].iloc[valid_indices].to_cupy()\n    prev_lon = df_gpu['prev_lon'].iloc[valid_indices].to_cupy()\n    merchant_lat = df_gpu['merchant_lat'].iloc[valid_indices].to_cupy()\n    merchant_lon = df_gpu['merchant_lon'].iloc[valid_indices].to_cupy()\n    \n    distances[valid_indices] = haversine_distance_gpu(\n        prev_lat, prev_lon, merchant_lat, merchant_lon\n    )\n\ndf_gpu['distance_from_prev_txn'] = distances\n\n# Calculate speed (km/h) - distance divided by time difference\n# Convert time_delta to cupy array\ndf_gpu['timeDelta'] = df_gpu['timeDelta'].fillna(0)\ntime_delta_array = df_gpu['timeDelta'].to_cupy()\nspeeds = cp.zeros_like(time_delta_array)\nvalid_time = time_delta_array > 0\nspeeds[valid_time] = distances[valid_time] / time_delta_array[valid_time]\ndf_gpu['speed_kmph'] = speeds\n\n# Calculate if transaction is in a different country from previous\ndf_gpu['prev_country'] = df_gpu.groupby('customerId')['merchantCountryCode'].shift(1)\ndf_gpu['different_country'] = (df_gpu['merchantCountryCode'] != df_gpu['prev_country']).astype('int32')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Additional Features\nprint(\"Engineering additional features...\")\n# Binary flags\ndf_gpu['cvv_match'] = (df_gpu['cardCVV'] == df_gpu['enteredCVV']).astype('int32')\ndf_gpu['exp_date_match'] = df_gpu['expirationDateKeyInMatch'].astype('int32')\ndf_gpu['is_foreign_transaction'] = (df_gpu['acqCountry'] != df_gpu['merchantCountryCode']).astype('int32')\n\n# Calculate the ratio of transaction amount to credit limit\ndf_gpu['amount_to_limit_ratio'] = df_gpu['transactionAmount'] / df_gpu['creditLimit'].fillna(1)\n\n# Calculate the ratio of transaction amount to available money\ndf_gpu['amount_to_available_ratio'] = df_gpu['transactionAmount'] / df_gpu['availableMoney'].fillna(1)\n\n# Calculate days since account opening\ndf_gpu['account_age_days'] = (df_gpu['transactionDateTime'] - df_gpu['accountOpenDate']).dt.days\n\n# Calculate days since last address change\ndf_gpu['days_since_address_change'] = (df_gpu['transactionDateTime'] - df_gpu['dateOfLastAddressChange']).dt.days\n\n# Is online transaction\ndf_gpu['isOnline'] = (df_gpu['merchantCategoryCode'] == 'online_retail').astype('int32')\n\n# One-hot encode categorical variables using GPU\nprint(\"One-hot encoding categorical variables...\")\ncategorical_cols = ['posEntryMode', 'posConditionCode', 'merchantCategoryCode', 'transactionType']\nfor col in categorical_cols:\n    dummies = cudf.get_dummies(df_gpu[col], prefix=col, drop_first=True)\n    df_gpu = df_gpu.join(dummies)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_gpu.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"GPU-accelerated feature engineering complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_gpu = df_gpu[[col for col in df_gpu.columns if col != 'isFraud'] + ['isFraud']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_gpu.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\n# Split into train and test set\ndf_train = df_gpu.iloc[:300000]\ndf_test = df_gpu.iloc[300000:]","metadata":{"_uuid":"35b92cb1-b33a-4cac-9950-dbb5f386b7dc","_cell_guid":"ff5d7f5c-8896-4a61-a6f2-49d390625626","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train=df_train.iloc[:, :-1]\ny_train=df_train.iloc[:, -1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Converted Training Data')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %load /usr/local/lib/python3.10/dist-packages/nodevectors/node2vec.py\nimport numba\nimport numpy as np\nimport pandas as pd\nimport time\nimport warnings\n\n# Gensim triggers automatic useless warnings for windows users...\nwarnings.simplefilter(\"ignore\", category=UserWarning)\nimport gensim\nwarnings.simplefilter(\"default\", category=UserWarning)\n\n\nimport csrgraph as cg\nfrom nodevectors.embedders import BaseNodeEmbedder\n\nclass Node2Vec(BaseNodeEmbedder):\n    def __init__(\n        self, \n        n_components=32,\n        walklen=30, \n        epochs=20,\n        return_weight=1.,\n        neighbor_weight=1.,\n        threads=0, \n        keep_walks=False,\n        verbose=True,\n        w2vparams={\"window\":10, \"negative\":5, \"iter\":10,\n                   \"batch_words\":128}):\n        \"\"\"\n        Parameters\n        ----------\n        walklen : int\n            length of the random walks\n        epochs : int\n            number of times to start a walk from each nodes\n        threads : int\n            number of threads to use. 0 is full use\n        n_components : int\n            number of resulting dimensions for the embedding\n            This should be set here rather than in the w2vparams arguments\n        return_weight : float in (0, inf]\n            Weight on the probability of returning to node coming from\n            Having this higher tends the walks to be \n            more like a Breadth-First Search.\n            Having this very high  (> 2) makes search very local.\n            Equal to the inverse of p in the Node2Vec paper.\n        neighbor_weight : float in (0, inf]\n            Weight on the probability of visitng a neighbor node\n            to the one we're coming from in the random walk\n            Having this higher tends the walks to be \n            more like a Depth-First Search.\n            Having this very high makes search more outward.\n            Having this very low makes search very local.\n            Equal to the inverse of q in the Node2Vec paper.\n        keep_walks : bool\n            Whether to save the random walks in the model object after training\n        w2vparams : dict\n            dictionary of parameters to pass to gensim's word2vec\n            Don't set the embedding dimensions through arguments here.\n        \"\"\"\n        if type(threads) is not int:\n            raise ValueError(\"Threads argument must be an int!\")\n        if walklen < 1 or epochs < 1:\n            raise ValueError(\"Walklen and epochs arguments must be > 1\")\n        self.n_components = n_components\n        self.walklen = walklen\n        self.epochs = epochs\n        self.keep_walks = keep_walks\n        if 'size' in w2vparams.keys():\n            raise AttributeError(\"Embedding dimensions should not be set \"\n                + \"through w2v parameters, but through n_components\")\n        self.w2vparams = w2vparams\n        self.return_weight = return_weight\n        self.neighbor_weight = neighbor_weight\n        if threads == 0:\n            threads = numba.config.NUMBA_DEFAULT_NUM_THREADS\n        self.threads = threads\n        w2vparams['workers'] = threads\n        self.verbose = verbose\n\n    def fit(self, G):\n        \"\"\"\n        NOTE: Currently only support str or int as node name for graph\n        Parameters\n        ----------\n        G : graph data\n            Graph to embed\n            Can be any graph type that's supported by csrgraph library\n            (NetworkX, numpy 2d array, scipy CSR matrix, CSR matrix components)\n        \"\"\"\n        if not isinstance(G, cg.csrgraph):\n            G = cg.csrgraph(G, threads=self.threads)\n        if G.threads != self.threads:\n            G.set_threads(self.threads)\n        # Because networkx graphs are actually iterables of their nodes\n        #   we do list(G) to avoid networkx 1.X vs 2.X errors\n        node_names = G.names\n        if type(node_names[0]) not in [int, str, np.int32, np.uint32, \n                                       np.int64, np.uint64]:\n            raise ValueError(\"Graph node names must be int or str!\")\n        # Adjacency matrix\n        walks_t = time.time()\n        if self.verbose:\n            print(\"Making walks...\", end=\" \")\n        self.walks = G.random_walks(walklen=self.walklen, \n                                    epochs=self.epochs,\n                                    return_weight=self.return_weight,\n                                    neighbor_weight=self.neighbor_weight)\n        if self.verbose:\n            print(f\"Done, T={time.time() - walks_t:.2f}\")\n            print(\"Mapping Walk Names...\", end=\" \")\n        map_t = time.time()\n        self.walks = pd.DataFrame(self.walks)\n        # Map nodeId -> node name\n        node_dict = dict(zip(np.arange(len(node_names)), node_names))\n        for col in self.walks.columns:\n            self.walks[col] = self.walks[col].map(node_dict).astype(str)\n        # Somehow gensim only trains on this list iterator\n        # it silently mistrains on array input\n        self.walks = [list(x) for x in self.walks.itertuples(False, None)]\n        if self.verbose:\n            print(f\"Done, T={time.time() - map_t:.2f}\")\n            print(\"Training W2V...\", end=\" \")\n            if gensim.models.word2vec.FAST_VERSION < 1:\n                print(\"WARNING: gensim word2vec version is unoptimized\"\n                    \"Try version 3.6 if on windows, versions 3.7 \"\n                    \"and 3.8 have had issues\")\n        w2v_t = time.time()\n        # Train gensim word2vec model on random walks\n        self.model = gensim.models.Word2Vec(\n            sentences=self.walks,\n            vector_size=self.n_components,\n            **self.w2vparams)\n        if not self.keep_walks:\n            del self.walks\n        if self.verbose:\n            print(f\"Done, T={time.time() - w2v_t:.2f}\")\n\n    def fit_transform(self, G):\n        \"\"\"\n        NOTE: Currently only support str or int as node name for graph\n        Parameters\n        ----------\n        G : graph data\n            Graph to embed\n            Can be any graph type that's supported by csrgraph library\n            (NetworkX, numpy 2d array, scipy CSR matrix, CSR matrix components)\n        \"\"\"\n        if not isinstance(G, cg.csrgraph):\n            G = cg.csrgraph(G, threads=self.threads)\n        self.fit(G)\n        w = np.array(\n            pd.DataFrame.from_records(\n            pd.Series(np.arange(len(G.nodes())))\n              .apply(self.predict)\n              .values)\n        )\n        return w\n    \n    def predict(self, node_name):\n        \"\"\"\n        Return vector associated with node\n        node_name : str or int\n            either the node ID or node name depending on graph format\n        \"\"\"\n        # current hack to work around word2vec problem\n        # ints need to be str -_-\n        if type(node_name) is not str:\n            node_name = str(node_name)\n        return self.model.wv.__getitem__(node_name)\n\n    def save_vectors(self, out_file):\n        \"\"\"\n        Save as embeddings in gensim.models.KeyedVectors format\n        \"\"\"\n        self.model.wv.save_word2vec_format(out_file)\n\n    def load_vectors(self, out_file):\n        \"\"\"\n        Load embeddings from gensim.models.KeyedVectors format\n        \"\"\"\n        self.model = gensim.wv.load_word2vec_format(out_file)","metadata":{"_uuid":"039ae2dd-550e-4a85-9d0b-df1be64605ae","_cell_guid":"b72e6246-1b68-4d47-8ade-de37e7959c7e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import multiprocessing\n\nworkers=multiprocessing.cpu_count()\nembedder = InductiveDeepwalk(dimensions=dimensions, walk_len = walk_len, walk_num=walk_num, workers=workers, verbose=0)","metadata":{"_uuid":"a8288319-2532-4584-9c70-bff1bef3c514","_cell_guid":"8afc8144-e84f-414b-996f-ef3b4f7e664f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train=X_train.to_pandas()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train=y_train.to_pandas()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Starting embedding')\nembedobj=embedder.fit(X_train, y_train)\nprint('ending embedding')","metadata":{"_uuid":"5f2093dc-ea29-434e-93d5-08884db572b5","_cell_guid":"ce6573f8-b323-4ea9-8080-55c998bb3f17","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params={'eval_metric' : ['auc','aucpr', 'logloss'],\n                          'n_estimators':300, \n                          'n_jobs':8, \n                          'learning_rate':0.1, \n                          'seed':42, \n                          'colsample_bytree' : 0.6,\n                          'colsample_bylevel':0.9, \n                          'subsample' : 0.9}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('embeddings.npy',embedobj.embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedobj.embeddings.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classifier = xgb.XGBClassifier(**params)","metadata":{"_uuid":"3cebe185-f3c1-45df-a2ff-b62fa9b230c1","_cell_guid":"4a3a7c62-8de5-4699-9be2-9a99e8c0a684","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classifier.fit(embedobj.embeddings[:300000],y_train[:300000])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_proba = classifier.predict_proba(embedobj.embeddings[300000:])","metadata":{"_uuid":"821da462-507a-4f53-a2a3-86fa754c7dd4","_cell_guid":"0dd2455e-9304-4529-8a74-69d86aa59655","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds=y_pred_proba[:, 1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{"_uuid":"ff269d4b-a9aa-43b3-bd71-0b92dfed6065","_cell_guid":"d61ce32a-088c-40a6-9f1c-3839916ec1d0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Assume y_proba[:, 1] contains probabilities for class 1\ny_pred = (preds >= 0.5).astype(int)  # Convert to binary labels (0 or 1)\n\n# Compute confusion matrix\ncm = confusion_matrix(y_train[300000:], y_pred)\n\nprint(cm)","metadata":{"_uuid":"7f796fc2-b787-401a-be9a-d76edf30f0b6","_cell_guid":"769304d4-d49c-40f7-b232-8c92cf0ef99e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}