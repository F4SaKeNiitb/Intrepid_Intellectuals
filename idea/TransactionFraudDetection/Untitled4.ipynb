{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from networkx.readwrite import edgelist\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_array, check_is_fitted\n",
        "import torch\n",
        "import networkx as nx\n",
        "from nodevectors import Node2Vec\n",
        "import pandas as pd\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from geopy.distance import geodesic\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "V8QYQWgARdUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inductive_pooling(edgelist, embeddings, G, workers, gamma=1000, dict_node=None,\n",
        "                     average_embedding=True, use_gpu=False, device=None):\n",
        "    \"\"\"\n",
        "    GPU-accelerated inductive pooling implementation\n",
        "    \"\"\"\n",
        "    # Convert edgelist to array\n",
        "    edgearray = np.array([[str(id), v[0], v[1]] for id, v in enumerate(edgelist)])\n",
        "\n",
        "    # Calculate average embedding\n",
        "    if average_embedding:\n",
        "        if use_gpu and device and device.type == 'cuda':\n",
        "            # Calculate on GPU\n",
        "            emb_tensor = torch.tensor(embeddings, device=device)\n",
        "            avg_emb = emb_tensor.mean(dim=0).cpu().numpy()\n",
        "        else:\n",
        "            # Calculate on CPU\n",
        "            avg_emb = embeddings.mean(axis=0)\n",
        "    else:\n",
        "        avg_emb = None\n",
        "\n",
        "    # Split processing based on GPU availability\n",
        "    if use_gpu and device and device.type == 'cuda' and len(edgearray) > 1000:\n",
        "        # For large datasets on GPU, we'll process in batches\n",
        "        if workers > 1:\n",
        "            print(\"Note: Using GPU with multiple workers. This may not be optimal for all systems.\")\n",
        "\n",
        "        result_list = []\n",
        "        split_arrays = np.array_split(edgearray, workers)\n",
        "\n",
        "        for batch in tqdm(split_arrays, total=len(split_arrays)):\n",
        "            result = gpu_inductive_pooling_batch(batch, embeddings, G, average_embedding=avg_emb, device=device)\n",
        "            result_list.append(result)\n",
        "    else:\n",
        "        # Use CPU multiprocessing for smaller datasets or if GPU is not available\n",
        "        result_list = []\n",
        "        with Pool(workers) as p:\n",
        "            for result in tqdm(p.imap(partial(inductive_pooling_chunk,\n",
        "                                             embeddings=embeddings,\n",
        "                                             G=G,\n",
        "                                             average_embedding=avg_emb),\n",
        "                                     np.array_split(edgearray, workers)),\n",
        "                              total=len(np.array_split(edgearray, workers))):\n",
        "                result_list.append(result)\n",
        "\n",
        "    # Combine results\n",
        "    new_embeddings = np.zeros((len(edgelist), embeddings.shape[1]))\n",
        "    for result_dict in result_list:\n",
        "        for id, emb in result_dict.items():\n",
        "            new_embeddings[int(id), :] = emb\n",
        "\n",
        "    return new_embeddings\n",
        "\n",
        "def gpu_inductive_pooling_batch(edgearray, embeddings, G, gamma=1000, average_embedding=None, device=None):\n",
        "    \"\"\"\n",
        "    GPU-accelerated version of inductive pooling for batch processing\n",
        "    \"\"\"\n",
        "    # Convert embeddings to GPU tensor if not already\n",
        "    if not torch.is_tensor(embeddings):\n",
        "        embeddings_tensor = torch.tensor(embeddings, device=device)\n",
        "    else:\n",
        "        embeddings_tensor = embeddings\n",
        "\n",
        "    # Create container for new embeddings\n",
        "    new_embeddings = dict()\n",
        "\n",
        "    for row in edgearray:\n",
        "        transfer, sender, receiver = row\n",
        "        mutual = False\n",
        "\n",
        "        if G.has_node(sender) and G.has_node(receiver):\n",
        "            mutual_neighbors = list(set(G.neighbors(sender)).intersection(set(G.neighbors(receiver))))\n",
        "            # Convert string ids to numerical ids\n",
        "            mutual_neighbors = list(map(int, mutual_neighbors))\n",
        "            # Sort numerical ids\n",
        "            mutual_neighbors.sort()\n",
        "\n",
        "            if len(mutual_neighbors) > 0:\n",
        "                mutual = True\n",
        "                # Take most recent mutual neighbor\n",
        "                most_recent_mutual_neighbor = mutual_neighbors[-1]\n",
        "                # Get embedding from GPU tensor\n",
        "                most_recent_embedding = embeddings_tensor[most_recent_mutual_neighbor].cpu().numpy()\n",
        "                new_embeddings[transfer] = most_recent_embedding\n",
        "\n",
        "        if G.has_node(sender) and (not mutual):\n",
        "            sender_neighbors = list(map(int, G.neighbors(sender)))\n",
        "            pooled_embedding = get_pooled_embedding_gpu(sender_neighbors, embeddings_tensor, gamma, device)\n",
        "            new_embeddings[transfer] = pooled_embedding\n",
        "\n",
        "        elif G.has_node(receiver) and (not mutual):\n",
        "            receiver_neighbors = list(map(int, G.neighbors(receiver)))\n",
        "            pooled_embedding = get_pooled_embedding_gpu(receiver_neighbors, embeddings_tensor, gamma, device)\n",
        "            new_embeddings[transfer] = pooled_embedding\n",
        "\n",
        "        elif not mutual:\n",
        "            # Use average embedding as fallback\n",
        "            if torch.is_tensor(average_embedding):\n",
        "                new_embeddings[transfer] = average_embedding.cpu().numpy()\n",
        "            else:\n",
        "                new_embeddings[transfer] = average_embedding\n",
        "\n",
        "    return new_embeddings\n",
        "\n",
        "def get_pooled_embedding_gpu(neighbors, embeddings_tensor, gamma, device):\n",
        "    \"\"\"\n",
        "    GPU-accelerated version of pooled embedding calculation\n",
        "    \"\"\"\n",
        "    # Extract embeddings for neighbors\n",
        "    if len(neighbors) == 0:\n",
        "        # Return zeros if no neighbors\n",
        "        return torch.zeros(embeddings_tensor.shape[1], device=device).cpu().numpy()\n",
        "\n",
        "    # Get indices for neighbors\n",
        "    indices = torch.tensor(neighbors, device=device)\n",
        "\n",
        "    # Use only the most recent gamma neighbors\n",
        "    start_idx = max(0, len(neighbors) - gamma)\n",
        "    indices = indices[start_idx:]\n",
        "\n",
        "    # Get embeddings for these neighbors\n",
        "    neighbor_embeddings = torch.index_select(embeddings_tensor, 0, indices)\n",
        "\n",
        "    # Calculate mean embedding\n",
        "    pooled_embedding = torch.mean(neighbor_embeddings, dim=0)\n",
        "\n",
        "    # Return as numpy array\n",
        "    return pooled_embedding.cpu().numpy()\n",
        "\n",
        "def inductive_pooling_chunk(edgearray, embeddings, G, gamma=1000, average_embedding=None):\n",
        "    \"\"\"\n",
        "    CPU version of inductive pooling for a chunk of edges\n",
        "    \"\"\"\n",
        "    # Create a container for the new embeddings\n",
        "    new_embeddings = dict()\n",
        "    for row in edgearray:\n",
        "        transfer, sender, receiver = row\n",
        "        mutual = False\n",
        "        if G.has_node(sender) and G.has_node(receiver):\n",
        "            mutual_neighbors = list(set(G.neighbors(sender)).intersection(set(G.neighbors(receiver))))\n",
        "            # convert string ids to numerical ids\n",
        "            mutual_neighbors = list(map(int, mutual_neighbors))\n",
        "            # sort numerical ids\n",
        "            mutual_neighbors.sort()\n",
        "\n",
        "            if (len(mutual_neighbors) > 0):\n",
        "                mutual = True\n",
        "                # take most recent mutual neighbor\n",
        "                most_recent_mutual_neighbor = mutual_neighbors[-1]\n",
        "                # Use dataframe with TX_ID on index (to speed up retrieval of transfer rows)\n",
        "                most_recent_embedding_mutual_neighbor = embeddings[most_recent_mutual_neighbor, :]\n",
        "                new_embeddings[transfer] = most_recent_embedding_mutual_neighbor\n",
        "\n",
        "\n",
        "        if G.has_node(sender) and (not mutual):\n",
        "            sender_neighbors = list(map(int, G.neighbors(sender)))\n",
        "            pooled_embedding = get_pooled_embedding(sender_neighbors, embeddings, gamma)\n",
        "\n",
        "            new_embeddings[transfer] = pooled_embedding\n",
        "\n",
        "        elif G.has_node(receiver) and (not mutual):\n",
        "            receiver_neighbors = list(map(int, G.neighbors(receiver)))\n",
        "            pooled_embedding = get_pooled_embedding(receiver_neighbors, embeddings, gamma)\n",
        "            new_embeddings[transfer] = pooled_embedding\n",
        "\n",
        "\n",
        "        elif (not mutual):\n",
        "            new_embeddings[transfer] = average_embedding\n",
        "\n",
        "    return new_embeddings\n",
        "\n",
        "def get_pooled_embedding(neighbors, embeddings, gamma):\n",
        "    \"\"\"\n",
        "    CPU version of pooled embedding calculation\n",
        "    \"\"\"\n",
        "    if len(neighbors) == 0:\n",
        "        # Return zeros if no neighbors\n",
        "        return np.zeros(embeddings.shape[1])\n",
        "\n",
        "    embeddings_to_pool = embeddings[neighbors, :]\n",
        "    most_recent_embeddings_to_pool = embeddings_to_pool[-min(gamma, embeddings_to_pool.shape[0]):, :]\n",
        "\n",
        "    pooled_embedding = most_recent_embeddings_to_pool.mean(axis=0)\n",
        "\n",
        "    return pooled_embedding\n",
        "\n",
        "# Assume EpochLogger implementation is needed\n",
        "class EpochLogger:\n",
        "    \"\"\"\n",
        "    Callback to log epoch progress\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        self.epoch += 1\n",
        "        print(f\"Completed epoch {self.epoch}\")"
      ],
      "metadata": {
        "id": "Atj8TeWWRiPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_network(X_train, y_train, use_gpu=True, batch_size=10000, verbose=True):\n",
        "    \"\"\"\n",
        "    GPU-accelerated function to create a network structure optimized for fraud detection.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : pandas.DataFrame\n",
        "        DataFrame containing transaction features\n",
        "    y_train : pandas.Series\n",
        "        Series containing fraud labels (1 for fraud, 0 for non-fraud)\n",
        "    use_gpu : bool, default=True\n",
        "        Whether to use GPU acceleration\n",
        "    batch_size : int, default=10000\n",
        "        Batch size for GPU operations\n",
        "    verbose : bool, default=True\n",
        "        Whether to show progress bars\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    G : networkx.Graph\n",
        "        Graph with nodes representing transactions, customers, merchants, and an artificial fraud node\n",
        "    \"\"\"\n",
        "    # Check if GPU is available when requested\n",
        "    if use_gpu:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        if verbose and device.type == 'cuda':\n",
        "            print(f\"Using GPU for network creation: {torch.cuda.get_device_name(0)}\")\n",
        "        elif verbose:\n",
        "            print(\"GPU requested but not available. Using CPU instead.\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        if verbose:\n",
        "            print(\"Using CPU as requested.\")\n",
        "\n",
        "    # Create graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Generate IDs\n",
        "    transaction_ids = [f\"txn_{i}\" for i in range(len(X_train))]\n",
        "    customer_ids = [f\"cust_{str(cid)}\" for cid in X_train['customerId']]\n",
        "    merchant_ids = [f\"merch_{name}_{country}\" for name, country in\n",
        "                  zip(X_train['merchantName'], X_train['merchantCountryCode'])]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Adding transaction nodes...\")\n",
        "\n",
        "    # Add nodes with attributes\n",
        "    for i, txn_id in enumerate(tqdm(transaction_ids) if verbose else transaction_ids):\n",
        "        # Add transaction node with relevant features\n",
        "        G.add_node(txn_id,\n",
        "                  type='transaction',\n",
        "                  amount_zscore=X_train.iloc[i]['amount_zscore'],\n",
        "                  amount_to_avg_ratio=X_train.iloc[i]['amount_to_avg_ratio'],\n",
        "                  is_foreign=X_train.iloc[i]['is_foreign_transaction'],\n",
        "                  cvv_match=X_train.iloc[i]['cvv_match'],\n",
        "                  exp_date_match=X_train.iloc[i]['exp_date_match'])\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Adding customer and merchant nodes...\")\n",
        "\n",
        "    # Add customer and merchant nodes\n",
        "    G.add_nodes_from(set(customer_ids), type='customer')\n",
        "    G.add_nodes_from(set(merchant_ids), type='merchant')\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Creating transaction-entity edges...\")\n",
        "\n",
        "    # Create edges between transactions and entities\n",
        "    for i, txn_id in enumerate(tqdm(transaction_ids) if verbose else transaction_ids):\n",
        "        # Connect transaction to customer\n",
        "        G.add_edge(txn_id, customer_ids[i], edge_type='customer_transaction')\n",
        "\n",
        "        # Connect transaction to merchant\n",
        "        G.add_edge(txn_id, merchant_ids[i], edge_type='merchant_transaction')\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Processing merchant proximity...\")\n",
        "\n",
        "    # Extract merchant coordinates\n",
        "    merchant_df = pd.DataFrame({\n",
        "        'merchant_id': merchant_ids,\n",
        "        'lat': X_train['merchant_lat'].values,\n",
        "        'lon': X_train['merchant_lon'].values\n",
        "    }).drop_duplicates('merchant_id')\n",
        "\n",
        "    # Filter out merchants with invalid coordinates\n",
        "    valid_merchant_df = merchant_df.dropna()\n",
        "\n",
        "    # GPU-accelerated geographical proximity calculation\n",
        "    if len(valid_merchant_df) > 0:\n",
        "        if use_gpu and device.type == 'cuda':\n",
        "            # Use GPU for proximity calculation\n",
        "            coords = torch.tensor(valid_merchant_df[['lat', 'lon']].values, device=device, dtype=torch.float32)\n",
        "\n",
        "            # Process in batches if dataset is large\n",
        "            merchant_edges = []\n",
        "            n_merchants = len(valid_merchant_df)\n",
        "\n",
        "            for i in range(0, n_merchants, batch_size):\n",
        "                end_idx = min(i + batch_size, n_merchants)\n",
        "                batch_coords = coords[i:end_idx]\n",
        "\n",
        "                # Calculate pairwise distances using GPU\n",
        "                # ||a - b||^2 = ||a||^2 + ||b||^2 - 2*a*b\n",
        "                a_norm = torch.sum(batch_coords**2, dim=1).view(-1, 1)\n",
        "                b_norm = torch.sum(coords**2, dim=1).view(1, -1)\n",
        "                dist_matrix = a_norm + b_norm - 2 * torch.mm(batch_coords, coords.t())\n",
        "                dist_matrix = torch.sqrt(torch.clamp(dist_matrix, min=0))\n",
        "\n",
        "                # Find close merchants\n",
        "                close_pairs = torch.nonzero(dist_matrix < 0.01, as_tuple=False)\n",
        "\n",
        "                # Add valid pairs to edges list\n",
        "                for pair in close_pairs:\n",
        "                    idx1, idx2 = pair[0].item() + i, pair[1].item()\n",
        "                    if idx1 != idx2:  # Avoid self-loops\n",
        "                        dist = dist_matrix[pair[0], pair[1]].item()\n",
        "                        m1 = valid_merchant_df.iloc[idx1]['merchant_id']\n",
        "                        m2 = valid_merchant_df.iloc[idx2]['merchant_id']\n",
        "                        merchant_edges.append((m1, m2, {'edge_type': 'location_proximity', 'weight': 1-dist*100}))\n",
        "\n",
        "            # Add all edges to graph\n",
        "            G.add_edges_from(merchant_edges)\n",
        "\n",
        "        else:\n",
        "            # CPU-based implementation using scikit-learn's NearestNeighbors\n",
        "            coords = valid_merchant_df[['lat', 'lon']].values\n",
        "\n",
        "            # Use NearestNeighbors for efficient proximity search\n",
        "            nbrs = NearestNeighbors(radius=0.01, algorithm='ball_tree').fit(coords)\n",
        "            distances, indices = nbrs.radius_neighbors(coords)\n",
        "\n",
        "            # Create edges for close merchants\n",
        "            merchant_edges = []\n",
        "            for i, idx_list in enumerate(indices):\n",
        "                for j, dist in zip(idx_list, distances[i]):\n",
        "                    if i != j:  # Avoid self-loops\n",
        "                        m1 = valid_merchant_df.iloc[i]['merchant_id']\n",
        "                        m2 = valid_merchant_df.iloc[j]['merchant_id']\n",
        "                        merchant_edges.append((m1, m2, {'edge_type': 'location_proximity', 'weight': 1-dist*100}))\n",
        "\n",
        "            # Add all edges to graph\n",
        "            G.add_edges_from(merchant_edges)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Processing sequential transactions...\")\n",
        "\n",
        "    # Create a mapping from customer to their transactions\n",
        "    customer_txn_map = {}\n",
        "    for i, cust_id in enumerate(customer_ids):\n",
        "        if cust_id not in customer_txn_map:\n",
        "            customer_txn_map[cust_id] = []\n",
        "        customer_txn_map[cust_id].append((transaction_ids[i], X_train.iloc[i]['timeDelta']))\n",
        "\n",
        "    # Connect transactions from the same customer if they occurred within a short time window\n",
        "    sequential_edges = []\n",
        "    for cust_id, txns in customer_txn_map.items():\n",
        "        if len(txns) > 1:\n",
        "            # Sort transactions by time\n",
        "            txns.sort(key=lambda x: x[1])\n",
        "\n",
        "            # Connect sequential transactions within 24 hours\n",
        "            for i in range(len(txns) - 1):\n",
        "                for j in range(i + 1, len(txns)):\n",
        "                    time_delta = abs(txns[j][1] - txns[i][1])\n",
        "                    if time_delta < 86400:  # Within 24 hours (in seconds)\n",
        "                        sequential_edges.append((txns[i][0], txns[j][0], {\n",
        "                            'edge_type': 'sequential_transactions',\n",
        "                            'time_diff': time_delta\n",
        "                        }))\n",
        "\n",
        "    # Add all sequential edges to graph\n",
        "    G.add_edges_from(sequential_edges)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Adding fraud node connections...\")\n",
        "\n",
        "    # Add artificial fraud node and connect it to all fraudulent transactions\n",
        "    fraud_node = \"ARTIFICIAL_FRAUD_NODE\"\n",
        "    G.add_node(fraud_node, type='artificial_fraud')\n",
        "\n",
        "    # Connect to fraudulent transactions\n",
        "    fraud_edges = [(txn_id, fraud_node, {'edge_type': 'is_fraud'})\n",
        "                  for i, txn_id in enumerate(transaction_ids) if y_train.iloc[i] == 1]\n",
        "    G.add_edges_from(fraud_edges)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Network creation complete. Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "\n",
        "    return G\n",
        "\n",
        "# Optional: A utility function to estimate memory requirements\n",
        "def estimate_network_memory(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Estimates the memory requirements for network creation\n",
        "    \"\"\"\n",
        "    n_transactions = len(X_train)\n",
        "    n_customers = X_train['customerId'].nunique()\n",
        "    n_merchants = X_train[['merchantName', 'merchantCountryCode']].drop_duplicates().shape[0]\n",
        "\n",
        "    # Estimate nodes (transactions, customers, merchants, fraud node)\n",
        "    n_nodes = n_transactions + n_customers + n_merchants + 1\n",
        "\n",
        "    # Estimate edges (transaction-customer, transaction-merchant, merchant proximity, sequential, fraud)\n",
        "    n_edges_base = n_transactions * 2  # Each transaction connects to a customer and merchant\n",
        "    n_fraud_edges = y_train.sum()\n",
        "\n",
        "    # Rough estimate of merchant proximity edges (assuming 5% of merchants are close)\n",
        "    n_merchant_proximity = int(n_merchants * n_merchants * 0.05)\n",
        "\n",
        "    # Rough estimate of sequential transaction edges (assuming 10% of transactions per customer are sequential)\n",
        "    avg_txn_per_customer = n_transactions / n_customers\n",
        "    n_sequential = int(n_customers * (avg_txn_per_customer * (avg_txn_per_customer - 1) / 2) * 0.1)\n",
        "\n",
        "    total_edges = n_edges_base + n_fraud_edges + n_merchant_proximity + n_sequential\n",
        "\n",
        "    # Estimate memory (rough approximation)\n",
        "    memory_per_node = 100  # bytes\n",
        "    memory_per_edge = 60   # bytes\n",
        "\n",
        "    estimated_memory = (n_nodes * memory_per_node + total_edges * memory_per_edge) / (1024 * 1024)  # in MB\n",
        "\n",
        "    return {\n",
        "        'n_nodes': n_nodes,\n",
        "        'n_edges': total_edges,\n",
        "        'estimated_memory_mb': estimated_memory,\n",
        "        'recommended_gpu_memory_gb': max(2, int(estimated_memory / 1024 * 3))  # 3x buffer\n",
        "    }"
      ],
      "metadata": {
        "id": "wlGsWLP0Ri4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "def check_edgelist(edgelist):\n",
        "\n",
        "    if not isinstance(edgelist, list):\n",
        "        edgelist = list(edgelist)\n",
        "\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "    '''Callback to log information about training'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_begin(self, model):\n",
        "        print(\"Epoch #{} start\".format(self.epoch))\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        print(\"Epoch #{} end\".format(self.epoch))\n",
        "        self.epoch += 1"
      ],
      "metadata": {
        "id": "BwoYLsw4Rnju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InductiveDeepwalk(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Implementation of DeepWalk with inductive capabilities for fraud detection.\n",
        "    GPU-accelerated version.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dimensions : int\n",
        "        Number of dimensions in the embeddings\n",
        "    walk_len : int\n",
        "        Length of each random walk\n",
        "    walk_num : int\n",
        "        Number of random walks per node\n",
        "    epochs : int, default=5\n",
        "        Number of training epochs\n",
        "    workers : int, default=1\n",
        "        Number of parallel workers\n",
        "    window_size : int, default=5\n",
        "        Context window size for Word2Vec\n",
        "    verbose : int, default=0\n",
        "        Verbosity level\n",
        "    use_gpu : bool, default=True\n",
        "        Whether to use GPU acceleration\n",
        "    \"\"\"\n",
        "    def __init__(self, dimensions, walk_len, walk_num, epochs=5, workers=1, window_size=5, verbose=0, use_gpu=True):\n",
        "        self.dimensions = dimensions\n",
        "        self.walk_len = walk_len\n",
        "        self.walk_num = walk_num\n",
        "        self.epochs = epochs\n",
        "        self.workers = workers\n",
        "        self.window_size = window_size\n",
        "        self.first_fit = True\n",
        "        self.verbose = verbose\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Check if GPU is available\n",
        "        if self.use_gpu:\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            if self.verbose > 0:\n",
        "                print(f\"Using device: {self.device}\")\n",
        "            if self.device.type == 'cpu' and self.use_gpu:\n",
        "                print(\"Warning: GPU requested but not available. Using CPU instead.\")\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            if self.verbose > 0:\n",
        "                print(\"Using CPU as requested.\")\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the model with X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas.DataFrame\n",
        "            Training data\n",
        "        y : pandas.Series\n",
        "            Target values (fraud labels)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self\n",
        "        \"\"\"\n",
        "        if self.verbose > 0:\n",
        "            print(\"Parsing input into network format.\")\n",
        "\n",
        "        # Create network using the updated function\n",
        "        self.G = create_network(X, y)\n",
        "\n",
        "        # Get transaction nodes\n",
        "        transaction_nodes = [n for n, d in self.G.nodes(data=True) if d.get('type') == 'transaction']\n",
        "\n",
        "        # Extract transaction IDs as integers for proper indexing\n",
        "        self.transaction_ids = [n.split('_')[1] for n in transaction_nodes]\n",
        "\n",
        "        callbacks = []\n",
        "        if self.verbose > 0:\n",
        "            print(\"Running network representation algorithm.\")\n",
        "            epochlogger = EpochLogger()\n",
        "            callbacks = [epochlogger]\n",
        "\n",
        "        # Configure Word2Vec parameters with GPU support if available\n",
        "        w2v_params = {\n",
        "            'workers': self.workers,\n",
        "            'window': self.window_size,\n",
        "            'callbacks': callbacks,\n",
        "            'compute_loss': True\n",
        "        }\n",
        "\n",
        "        # Train Node2Vec model\n",
        "        g2v = Node2Vec(\n",
        "            n_components=self.dimensions,\n",
        "            walklen=self.walk_len,\n",
        "            epochs=self.walk_num,\n",
        "            verbose=self.verbose,\n",
        "            w2vparams=w2v_params\n",
        "        )\n",
        "        g2v.fit(self.G)\n",
        "        self.model = g2v.model\n",
        "\n",
        "        # Create dictionary of node embeddings\n",
        "        self.node_embeddings = {}\n",
        "        for node in self.G.nodes():\n",
        "            try:\n",
        "                self.node_embeddings[node] = self.model.wv[node]\n",
        "            except KeyError:\n",
        "                # Handle nodes not in vocabulary\n",
        "                self.node_embeddings[node] = np.zeros(self.dimensions)\n",
        "\n",
        "        # Create array of transaction embeddings for easier access\n",
        "        self.embeddings = np.zeros((len(transaction_nodes), self.dimensions))\n",
        "        for i, txn_id in enumerate(transaction_nodes):\n",
        "            self.embeddings[i] = self.node_embeddings[txn_id]\n",
        "\n",
        "        # Convert embeddings to PyTorch tensors for GPU processing\n",
        "        if self.use_gpu and self.device.type == 'cuda':\n",
        "            self.embeddings_tensor = torch.tensor(self.embeddings, device=self.device)\n",
        "\n",
        "        self.is_fitted_ = True\n",
        "        self.first_fit = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas.DataFrame\n",
        "            Test data containing transaction features\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Transaction embeddings\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "\n",
        "        if self.first_fit:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Retrieving embeddings for training data.\")\n",
        "            results = self.embeddings\n",
        "            self.first_fit = False\n",
        "        else:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Running inductive pooling extension.\")\n",
        "            results = inductive_pooling(X, self.embeddings, self.G, workers=self.workers,\n",
        "                                       use_gpu=self.use_gpu, device=self.device)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "98R5Em0tRpGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "dimensions = 32\n",
        "walk_len = 80\n",
        "walk_num = 10\n",
        "window_size = 5\n",
        "# the 'workers' parameter is used for multi-processing.\n",
        "workers = 4"
      ],
      "metadata": {
        "id": "dpv2I7byRxPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI4GHUqbRbZ6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split into train and test set\n",
        "df_train = pd.read_csv('/content/NewDatFrame.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=df_train.iloc[:, :-1]\n",
        "y_train=df_train.iloc[:, -1]"
      ],
      "metadata": {
        "id": "vbVdis73R2Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load /usr/local/lib/python3.10/dist-packages/nodevectors/node2vec.py\n",
        "import numba\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Gensim triggers automatic useless warnings for windows users...\n",
        "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "import gensim\n",
        "warnings.simplefilter(\"default\", category=UserWarning)\n",
        "\n",
        "\n",
        "import csrgraph as cg\n",
        "from nodevectors.embedders import BaseNodeEmbedder\n",
        "\n",
        "class Node2Vec(BaseNodeEmbedder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_components=32,\n",
        "        walklen=30,\n",
        "        epochs=20,\n",
        "        return_weight=1.,\n",
        "        neighbor_weight=1.,\n",
        "        threads=0,\n",
        "        keep_walks=False,\n",
        "        verbose=True,\n",
        "        w2vparams={\"window\":10, \"negative\":5, \"iter\":10,\n",
        "                   \"batch_words\":128}):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        walklen : int\n",
        "            length of the random walks\n",
        "        epochs : int\n",
        "            number of times to start a walk from each nodes\n",
        "        threads : int\n",
        "            number of threads to use. 0 is full use\n",
        "        n_components : int\n",
        "            number of resulting dimensions for the embedding\n",
        "            This should be set here rather than in the w2vparams arguments\n",
        "        return_weight : float in (0, inf]\n",
        "            Weight on the probability of returning to node coming from\n",
        "            Having this higher tends the walks to be\n",
        "            more like a Breadth-First Search.\n",
        "            Having this very high  (> 2) makes search very local.\n",
        "            Equal to the inverse of p in the Node2Vec paper.\n",
        "        neighbor_weight : float in (0, inf]\n",
        "            Weight on the probability of visitng a neighbor node\n",
        "            to the one we're coming from in the random walk\n",
        "            Having this higher tends the walks to be\n",
        "            more like a Depth-First Search.\n",
        "            Having this very high makes search more outward.\n",
        "            Having this very low makes search very local.\n",
        "            Equal to the inverse of q in the Node2Vec paper.\n",
        "        keep_walks : bool\n",
        "            Whether to save the random walks in the model object after training\n",
        "        w2vparams : dict\n",
        "            dictionary of parameters to pass to gensim's word2vec\n",
        "            Don't set the embedding dimensions through arguments here.\n",
        "        \"\"\"\n",
        "        if type(threads) is not int:\n",
        "            raise ValueError(\"Threads argument must be an int!\")\n",
        "        if walklen < 1 or epochs < 1:\n",
        "            raise ValueError(\"Walklen and epochs arguments must be > 1\")\n",
        "        self.n_components = n_components\n",
        "        self.walklen = walklen\n",
        "        self.epochs = epochs\n",
        "        self.keep_walks = keep_walks\n",
        "        if 'size' in w2vparams.keys():\n",
        "            raise AttributeError(\"Embedding dimensions should not be set \"\n",
        "                + \"through w2v parameters, but through n_components\")\n",
        "        self.w2vparams = w2vparams\n",
        "        self.return_weight = return_weight\n",
        "        self.neighbor_weight = neighbor_weight\n",
        "        if threads == 0:\n",
        "            threads = numba.config.NUMBA_DEFAULT_NUM_THREADS\n",
        "        self.threads = threads\n",
        "        w2vparams['workers'] = threads\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, G):\n",
        "        \"\"\"\n",
        "        NOTE: Currently only support str or int as node name for graph\n",
        "        Parameters\n",
        "        ----------\n",
        "        G : graph data\n",
        "            Graph to embed\n",
        "            Can be any graph type that's supported by csrgraph library\n",
        "            (NetworkX, numpy 2d array, scipy CSR matrix, CSR matrix components)\n",
        "        \"\"\"\n",
        "        if not isinstance(G, cg.csrgraph):\n",
        "            G = cg.csrgraph(G, threads=self.threads)\n",
        "        if G.threads != self.threads:\n",
        "            G.set_threads(self.threads)\n",
        "        # Because networkx graphs are actually iterables of their nodes\n",
        "        #   we do list(G) to avoid networkx 1.X vs 2.X errors\n",
        "        node_names = G.names\n",
        "        if type(node_names[0]) not in [int, str, np.int32, np.uint32,\n",
        "                                       np.int64, np.uint64]:\n",
        "            raise ValueError(\"Graph node names must be int or str!\")\n",
        "        # Adjacency matrix\n",
        "        walks_t = time.time()\n",
        "        if self.verbose:\n",
        "            print(\"Making walks...\", end=\" \")\n",
        "        self.walks = G.random_walks(walklen=self.walklen,\n",
        "                                    epochs=self.epochs,\n",
        "                                    return_weight=self.return_weight,\n",
        "                                    neighbor_weight=self.neighbor_weight)\n",
        "        if self.verbose:\n",
        "            print(f\"Done, T={time.time() - walks_t:.2f}\")\n",
        "            print(\"Mapping Walk Names...\", end=\" \")\n",
        "        map_t = time.time()\n",
        "        self.walks = pd.DataFrame(self.walks)\n",
        "        # Map nodeId -> node name\n",
        "        node_dict = dict(zip(np.arange(len(node_names)), node_names))\n",
        "        for col in self.walks.columns:\n",
        "            self.walks[col] = self.walks[col].map(node_dict).astype(str)\n",
        "        # Somehow gensim only trains on this list iterator\n",
        "        # it silently mistrains on array input\n",
        "        self.walks = [list(x) for x in self.walks.itertuples(False, None)]\n",
        "        if self.verbose:\n",
        "            print(f\"Done, T={time.time() - map_t:.2f}\")\n",
        "            print(\"Training W2V...\", end=\" \")\n",
        "            if gensim.models.word2vec.FAST_VERSION < 1:\n",
        "                print(\"WARNING: gensim word2vec version is unoptimized\"\n",
        "                    \"Try version 3.6 if on windows, versions 3.7 \"\n",
        "                    \"and 3.8 have had issues\")\n",
        "        w2v_t = time.time()\n",
        "        # Train gensim word2vec model on random walks\n",
        "        self.model = gensim.models.Word2Vec(\n",
        "            sentences=self.walks,\n",
        "            vector_size=self.n_components,\n",
        "            **self.w2vparams)\n",
        "        if not self.keep_walks:\n",
        "            del self.walks\n",
        "        if self.verbose:\n",
        "            print(f\"Done, T={time.time() - w2v_t:.2f}\")\n",
        "\n",
        "    def fit_transform(self, G):\n",
        "        \"\"\"\n",
        "        NOTE: Currently only support str or int as node name for graph\n",
        "        Parameters\n",
        "        ----------\n",
        "        G : graph data\n",
        "            Graph to embed\n",
        "            Can be any graph type that's supported by csrgraph library\n",
        "            (NetworkX, numpy 2d array, scipy CSR matrix, CSR matrix components)\n",
        "        \"\"\"\n",
        "        if not isinstance(G, cg.csrgraph):\n",
        "            G = cg.csrgraph(G, threads=self.threads)\n",
        "        self.fit(G)\n",
        "        w = np.array(\n",
        "            pd.DataFrame.from_records(\n",
        "            pd.Series(np.arange(len(G.nodes())))\n",
        "              .apply(self.predict)\n",
        "              .values)\n",
        "        )\n",
        "        return w\n",
        "\n",
        "    def predict(self, node_name):\n",
        "        \"\"\"\n",
        "        Return vector associated with node\n",
        "        node_name : str or int\n",
        "            either the node ID or node name depending on graph format\n",
        "        \"\"\"\n",
        "        # current hack to work around word2vec problem\n",
        "        # ints need to be str -_-\n",
        "        if type(node_name) is not str:\n",
        "            node_name = str(node_name)\n",
        "        return self.model.wv.__getitem__(node_name)\n",
        "\n",
        "    def save_vectors(self, out_file):\n",
        "        \"\"\"\n",
        "        Save as embeddings in gensim.models.KeyedVectors format\n",
        "        \"\"\"\n",
        "        self.model.wv.save_word2vec_format(out_file)\n",
        "\n",
        "    def load_vectors(self, out_file):\n",
        "        \"\"\"\n",
        "        Load embeddings from gensim.models.KeyedVectors format\n",
        "        \"\"\"\n",
        "        self.model = gensim.wv.load_word2vec_format(out_file)"
      ],
      "metadata": {
        "id": "bsuynBj0R4m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "workers=multiprocessing.cpu_count()\n",
        "embedder = InductiveDeepwalk(dimensions=dimensions, walk_len = walk_len, walk_num=walk_num, workers=workers, verbose=0)"
      ],
      "metadata": {
        "id": "dEv3nF9IR5Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Starting embedding')\n",
        "embedobj=embedder.fit(X_train, y_train)\n",
        "print('ending embedding')"
      ],
      "metadata": {
        "id": "jAOyLbQ-R8mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('embeddings.npy',embedobj.embeddings)"
      ],
      "metadata": {
        "id": "-OEZJuzlR_-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}