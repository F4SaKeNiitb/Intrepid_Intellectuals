{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11056614,"sourceType":"datasetVersion","datasetId":6888547}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nimport optuna\nimport cuml\nfrom cuml.ensemble import IsolationForest as cuIsolationForest\nimport shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/newdataframe/NewDatFrame.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T06:09:39.397517Z","iopub.execute_input":"2025-03-17T06:09:39.397908Z","iopub.status.idle":"2025-03-17T06:09:42.792528Z","shell.execute_reply.started":"2025-03-17T06:09:39.397869Z","shell.execute_reply":"2025-03-17T06:09:42.791461Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T06:09:42.793452Z","iopub.execute_input":"2025-03-17T06:09:42.793719Z","iopub.status.idle":"2025-03-17T06:09:42.800176Z","shell.execute_reply.started":"2025-03-17T06:09:42.793690Z","shell.execute_reply":"2025-03-17T06:09:42.799178Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0.1', 'Unnamed: 0', 'accountNumber', 'customerId',\n       'creditLimit', 'availableMoney', 'transactionDateTime',\n       'transactionAmount', 'merchantName', 'acqCountry',\n       'merchantCountryCode', 'posEntryMode', 'posConditionCode',\n       'merchantCategoryCode', 'currentExpDate', 'accountOpenDate',\n       'dateOfLastAddressChange', 'cardCVV', 'enteredCVV', 'cardLast4Digits',\n       'transactionType', 'echoBuffer', 'currentBalance', 'merchantCity',\n       'merchantState', 'merchantZip', 'cardPresent', 'posOnPremises',\n       'recurringAuthInd', 'expirationDateKeyInMatch', 'merchant_loc',\n       'merchant_lat', 'merchant_lon', 'prevTransactionTime', 'timeDelta',\n       'txn_count_24h', 'txn_count_7d', 'amount_zscore', 'amount_to_avg_ratio',\n       'amount_24h', 'prev_lat', 'prev_lon', 'distance_from_prev_txn',\n       'speed_kmph', 'prev_country', 'different_country', 'cvv_match',\n       'exp_date_match', 'is_foreign_transaction', 'amount_to_limit_ratio',\n       'amount_to_available_ratio', 'account_age_days',\n       'days_since_address_change', 'isOnline', 'isFraud'],\n      dtype='object')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def preprocess_data(X_train):\n    # Drop columns that are:\n    # - Direct identifiers (PII)\n    # - Redundant features\n    # - Features that would lead to data leakage\n    # - Features with high cardinality that need special encoding\n    \n    columns_to_drop = [\n        # Identifiers\n        'Unnamed: 0.1', 'Unnamed: 0','accountNumber', 'customerId', 'cardLast4Digits', 'cardCVV', 'enteredCVV',\n        \n        # Date/time columns (already processed into derived features)\n        'transactionDateTime', 'currentExpDate', 'accountOpenDate', 'dateOfLastAddressChange',\n        \n        # Raw location data (already processed into distance/speed)\n        'merchantCity', 'merchantState', 'merchantZip', 'merchant_loc', 'merchant_lat', 'merchant_lon',\n        \n        # Echo/buffer data (likely technical metadata)\n        'echoBuffer',\n        \n        # Features derived from other features that are kept\n        'prevTransactionTime' \n    ]\n    \n    # Keep only the relevant columns\n    X_clean = X_train.drop(columns=columns_to_drop, errors='ignore')\n    \n    # Convert categorical features to one-hot encoding\n    categorical_cols = ['merchantName', 'acqCountry', 'merchantCountryCode', 'posEntryMode', \n                      'posConditionCode', 'merchantCategoryCode', 'transactionType', 'prev_country']\n    \n    for col in categorical_cols:\n        if col in X_clean.columns:\n            # For high cardinality features, consider frequency encoding instead\n            if X_clean[col].nunique() > 20:\n                # Replace with frequency encoding\n                freq = X_clean[col].value_counts(normalize=True)\n                X_clean[col] = X_clean[col].map(freq)\n            else:\n                # One-hot encode\n                X_clean = pd.get_dummies(X_clean, columns=[col], prefix=col, drop_first=True)\n    \n    # Fill any missing values\n    X_clean = X_clean.fillna(0)\n    \n    return X_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T06:09:57.555641Z","iopub.status.idle":"2025-03-17T06:09:57.555981Z","shell.execute_reply":"2025-03-17T06:09:57.555840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Set up hyperparameter optimization with Optuna\ndef objective(trial, X_train_processed, X_val=None, y_val=None):\n    # Define hyperparameters to optimize\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n        'contamination': trial.suggest_float('contamination', 0.01, 0.2),\n        'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n        'n_jobs': -1,  # Use all CPU cores\n        'random_state': 42,\n        'verbose': 0\n    }\n    \n    # Use GPU implementation if possible, otherwise fallback to CPU\n    try:\n        # For cuML, we need to adjust parameters slightly\n        cuml_params = params.copy()\n        cuml_params.pop('n_jobs', None)  # cuML doesn't use n_jobs\n        cuml_params.pop('verbose', None)  # Not needed in cuML\n        \n        # Train Isolation Forest model using GPU\n        model = cuIsolationForest(**cuml_params)\n    except (ImportError, Exception) as e:\n        print(f\"Using CPU implementation due to: {e}\")\n        model = IsolationForest(**params)\n    \n    # Fit the model\n    model.fit(X_train_processed)\n    \n    # Evaluate the model\n    # For anomaly detection, we'll use the negative anomaly score as the metric\n    # Lower anomaly scores indicate more anomalous instances\n    if X_val is not None and y_val is not None:\n        # Get anomaly scores\n        scores = -model.score_samples(X_val)\n        \n        # Calculate AUC\n        from sklearn.metrics import roc_auc_score\n        auc = roc_auc_score(y_val, scores)\n        return auc\n    else:\n        # If no validation set, use the training set anomaly scores distribution\n        scores = -model.score_samples(X_train_processed)\n        # Return the separation between anomaly scores (higher is better)\n        return np.std(scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Main function to orchestrate the process\ndef train_isolation_forest(X_train, y_train, n_trials=50):\n    # Split data into train and validation sets\n    from sklearn.model_selection import train_test_split\n    X_train_split, X_val, y_train_split, y_val = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n    )\n    \n    # Preprocess the data\n    print(\"Preprocessing data...\")\n    X_train_processed = preprocess_data(X_train_split)\n    X_val_processed = preprocess_data(X_val)\n    \n    # Scale the data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train_processed)\n    X_val_scaled = scaler.transform(X_val_processed)\n    \n    # Create an Optuna study for hyperparameter optimization\n    print(\"Starting hyperparameter optimization...\")\n    study = optuna.create_study(direction='maximize', study_name='isolation_forest_optimization')\n    \n    # Optimize the hyperparameters\n    study.optimize(lambda trial: objective(trial, X_train_scaled, X_val_scaled, y_val), n_trials=n_trials)\n    \n    # Get the best hyperparameters\n    best_params = study.best_params\n    print(f\"Best parameters: {best_params}\")\n    print(f\"Best value (AUC): {study.best_value:.4f}\")\n    \n    # Get the top 5 most important hyperparameters\n    param_importances = optuna.importance.get_param_importances(study)\n    print(\"\\nTop 5 Hyperparameter Importances:\")\n    for param, importance in sorted(param_importances.items(), key=lambda x: x[1], reverse=True)[:5]:\n        print(f\"{param}: {importance:.4f}\")\n    \n    # Train the final model with the best hyperparameters\n    print(\"\\nTraining final model with best parameters...\")\n    try:\n        final_params = best_params.copy()\n        final_params.pop('n_jobs', None)\n        final_params.pop('verbose', None)\n        final_model = cuIsolationForest(**final_params, random_state=42)\n    except:\n        final_params = best_params.copy()\n        final_params['n_jobs'] = -1\n        final_params['verbose'] = 0\n        final_model = IsolationForest(**final_params, random_state=42)\n    \n    # Combine training and validation for final training\n    X_combined = np.vstack([X_train_scaled, X_val_scaled])\n    final_model.fit(X_combined)\n    \n    # Feature importance analysis\n    try:\n        # For feature importance, we use SHAP values if possible\n        print(\"\\nCalculating feature importance...\")\n        X_importance = pd.DataFrame(X_combined, columns=X_train_processed.columns)\n        explainer = shap.Explainer(final_model, X_importance)\n        shap_values = explainer(X_importance)\n        \n        # Get mean absolute SHAP value for each feature as importance\n        feature_importance = pd.DataFrame({\n            'Feature': X_train_processed.columns,\n            'Importance': np.abs(shap_values.values).mean(axis=0)\n        })\n        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n        \n        print(\"\\nTop 10 Most Important Features:\")\n        print(feature_importance.head(10))\n    except Exception as e:\n        print(f\"Could not calculate feature importance: {e}\")\n    \n    return final_model, best_params, param_importances, study","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, best_params, param_importances, study = train_isolation_forest(X_train, y_train, n_trials=50)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}