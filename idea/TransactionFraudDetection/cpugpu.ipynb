{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10955452,"sourceType":"datasetVersion","datasetId":6815388},{"sourceId":10970341,"sourceType":"datasetVersion","datasetId":6825981},{"sourceId":11041173,"sourceType":"datasetVersion","datasetId":6877526},{"sourceId":11052617,"sourceType":"datasetVersion","datasetId":6885846}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CATCHM pipeline [demo]","metadata":{"_uuid":"93f298fe-fe00-4078-95f7-c6f9f69a24f4","_cell_guid":"30acb7ce-0df3-453d-a44a-6df3b06282b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"This notebook is in many ways identical to the *CATCHM_demo.ipynb* notebook.\nHowever, in this notebook CATCHM is implemented as a [ScikitLearn compatible pipeline object](https://scikit-learn.org/stable/modules/compose.html). \nThis allows you to experiment with different classifiers and replace the default [XGBoost model](https://xgboost.readthedocs.io/en/stable/python/index.html).","metadata":{"_uuid":"98f60aed-1fe9-4ca5-b77f-5b21bf7b35a6","_cell_guid":"d2363334-556d-4188-8ec6-7fe7abdc082d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install --upgrade numpy pandas nodevectors xgboost fucc optuna scikit-learn cupy-cuda12x numba scipy==1.11.4 networkx==2.5.1","metadata":{"_uuid":"178aba82-fdcf-41e5-a0c8-cd36de801aa1","_cell_guid":"df2a6484-2d0c-4753-8fa9-9d194fdeb99e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:54:18.104229Z","iopub.execute_input":"2025-03-16T17:54:18.104556Z","iopub.status.idle":"2025-03-16T17:54:27.382217Z","shell.execute_reply.started":"2025-03-16T17:54:18.104534Z","shell.execute_reply":"2025-03-16T17:54:27.381327Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nCollecting numpy\n  Using cached numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: nodevectors in /usr/local/lib/python3.10/dist-packages (0.1.23)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (3.0.0)\nRequirement already satisfied: fucc in /usr/local/lib/python3.10/dist-packages (0.0.8)\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.1)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (13.4.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.61.0)\nRequirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.10/dist-packages (1.11.4)\nRequirement already satisfied: networkx==2.5.1 in /usr/local/lib/python3.10/dist-packages (2.5.1)\nRequirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from networkx==2.5.1) (4.4.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: csrgraph in /usr/local/lib/python3.10/dist-packages (from nodevectors) (0.1.28)\nRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from nodevectors) (4.3.3)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fucc) (4.67.1)\nRequirement already satisfied: scikit-plot in /usr/local/lib/python3.10/dist-packages (from fucc) (0.3.7)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fucc) (3.7.5)\nRequirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (from fucc) (1.2.1)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x) (0.8.2)\nRequirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.44.0)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.10/dist-packages (from dateparser->fucc) (2024.11.6)\nRequirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.10/dist-packages (from dateparser->fucc) (5.2)\nRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->nodevectors) (7.0.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fucc) (3.2.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->nodevectors) (1.17.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from networkx.readwrite import edgelist\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\nimport torch\nimport networkx as nx\nfrom nodevectors import Node2Vec\nimport pandas as pd\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:54:27.383374Z","iopub.execute_input":"2025-03-16T17:54:27.383605Z","iopub.status.idle":"2025-03-16T17:54:45.653000Z","shell.execute_reply.started":"2025-03-16T17:54:27.383585Z","shell.execute_reply":"2025-03-16T17:54:45.652324Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def inductive_pooling(edgelist, embeddings, G, workers, gamma=1000, dict_node=None, \n                     average_embedding=True, use_gpu=False, device=None):\n    \"\"\"\n    GPU-accelerated inductive pooling implementation\n    \"\"\"\n    # Convert edgelist to array\n    edgearray = np.array([[str(id), v[0], v[1]] for id, v in enumerate(edgelist)])\n    \n    # Calculate average embedding\n    if average_embedding:\n        if use_gpu and device and device.type == 'cuda':\n            # Calculate on GPU\n            emb_tensor = torch.tensor(embeddings, device=device)\n            avg_emb = emb_tensor.mean(dim=0).cpu().numpy()\n        else:\n            # Calculate on CPU\n            avg_emb = embeddings.mean(axis=0)\n    else:\n        avg_emb = None\n    \n    # Split processing based on GPU availability\n    if use_gpu and device and device.type == 'cuda' and len(edgearray) > 1000:\n        # For large datasets on GPU, we'll process in batches\n        if workers > 1:\n            print(\"Note: Using GPU with multiple workers. This may not be optimal for all systems.\")\n        \n        result_list = []\n        split_arrays = np.array_split(edgearray, workers)\n        \n        for batch in tqdm(split_arrays, total=len(split_arrays)):\n            result = gpu_inductive_pooling_batch(batch, embeddings, G, average_embedding=avg_emb, device=device)\n            result_list.append(result)\n    else:\n        # Use CPU multiprocessing for smaller datasets or if GPU is not available\n        result_list = []\n        with Pool(workers) as p:\n            for result in tqdm(p.imap(partial(inductive_pooling_chunk, \n                                             embeddings=embeddings, \n                                             G=G, \n                                             average_embedding=avg_emb), \n                                     np.array_split(edgearray, workers)), \n                              total=len(np.array_split(edgearray, workers))):\n                result_list.append(result)\n    \n    # Combine results\n    new_embeddings = np.zeros((len(edgelist), embeddings.shape[1]))\n    for result_dict in result_list:\n        for id, emb in result_dict.items():\n            new_embeddings[int(id), :] = emb\n    \n    return new_embeddings\n\ndef gpu_inductive_pooling_batch(edgearray, embeddings, G, gamma=1000, average_embedding=None, device=None):\n    \"\"\"\n    GPU-accelerated version of inductive pooling for batch processing\n    \"\"\"\n    # Convert embeddings to GPU tensor if not already\n    if not torch.is_tensor(embeddings):\n        embeddings_tensor = torch.tensor(embeddings, device=device)\n    else:\n        embeddings_tensor = embeddings\n        \n    # Create container for new embeddings\n    new_embeddings = dict()\n    \n    for row in edgearray:\n        transfer, sender, receiver = row\n        mutual = False\n        \n        if G.has_node(sender) and G.has_node(receiver):\n            mutual_neighbors = list(set(G.neighbors(sender)).intersection(set(G.neighbors(receiver))))\n            # Convert string ids to numerical ids \n            mutual_neighbors = list(map(int, mutual_neighbors))\n            # Sort numerical ids\n            mutual_neighbors.sort()\n            \n            if len(mutual_neighbors) > 0:\n                mutual = True\n                # Take most recent mutual neighbor\n                most_recent_mutual_neighbor = mutual_neighbors[-1]\n                # Get embedding from GPU tensor\n                most_recent_embedding = embeddings_tensor[most_recent_mutual_neighbor].cpu().numpy()\n                new_embeddings[transfer] = most_recent_embedding\n                \n        if G.has_node(sender) and (not mutual):\n            sender_neighbors = list(map(int, G.neighbors(sender)))\n            pooled_embedding = get_pooled_embedding_gpu(sender_neighbors, embeddings_tensor, gamma, device)\n            new_embeddings[transfer] = pooled_embedding\n            \n        elif G.has_node(receiver) and (not mutual):\n            receiver_neighbors = list(map(int, G.neighbors(receiver)))\n            pooled_embedding = get_pooled_embedding_gpu(receiver_neighbors, embeddings_tensor, gamma, device)\n            new_embeddings[transfer] = pooled_embedding\n            \n        elif not mutual:\n            # Use average embedding as fallback\n            if torch.is_tensor(average_embedding):\n                new_embeddings[transfer] = average_embedding.cpu().numpy()\n            else:\n                new_embeddings[transfer] = average_embedding\n                \n    return new_embeddings\n\ndef get_pooled_embedding_gpu(neighbors, embeddings_tensor, gamma, device):\n    \"\"\"\n    GPU-accelerated version of pooled embedding calculation\n    \"\"\"\n    # Extract embeddings for neighbors\n    if len(neighbors) == 0:\n        # Return zeros if no neighbors\n        return torch.zeros(embeddings_tensor.shape[1], device=device).cpu().numpy()\n    \n    # Get indices for neighbors\n    indices = torch.tensor(neighbors, device=device)\n    \n    # Use only the most recent gamma neighbors\n    start_idx = max(0, len(neighbors) - gamma)\n    indices = indices[start_idx:]\n    \n    # Get embeddings for these neighbors\n    neighbor_embeddings = torch.index_select(embeddings_tensor, 0, indices)\n    \n    # Calculate mean embedding\n    pooled_embedding = torch.mean(neighbor_embeddings, dim=0)\n    \n    # Return as numpy array\n    return pooled_embedding.cpu().numpy()\n\ndef inductive_pooling_chunk(edgearray, embeddings, G, gamma=1000, average_embedding=None):\n    \"\"\"\n    CPU version of inductive pooling for a chunk of edges\n    \"\"\"\n    # Create a container for the new embeddings\n    new_embeddings = dict()\n    for row in edgearray:\n        transfer, sender, receiver = row\n        mutual = False    \n        if G.has_node(sender) and G.has_node(receiver):\n            mutual_neighbors = list(set(G.neighbors(sender)).intersection(set(G.neighbors(receiver))))\n            # convert string ids to numerical ids \n            mutual_neighbors = list(map(int, mutual_neighbors))\n            # sort numerical ids\n            mutual_neighbors.sort()\n            \n            if (len(mutual_neighbors) > 0): \n                mutual = True\n                # take most recent mutual neighbor\n                most_recent_mutual_neighbor = mutual_neighbors[-1]\n                # Use dataframe with TX_ID on index (to speed up retrieval of transfer rows)\n                most_recent_embedding_mutual_neighbor = embeddings[most_recent_mutual_neighbor, :]\n                new_embeddings[transfer] = most_recent_embedding_mutual_neighbor\n                \n                        \n        if G.has_node(sender) and (not mutual):\n            sender_neighbors = list(map(int, G.neighbors(sender)))\n            pooled_embedding = get_pooled_embedding(sender_neighbors, embeddings, gamma)\n            \n            new_embeddings[transfer] = pooled_embedding\n            \n        elif G.has_node(receiver) and (not mutual):\n            receiver_neighbors = list(map(int, G.neighbors(receiver)))\n            pooled_embedding = get_pooled_embedding(receiver_neighbors, embeddings, gamma)\n            new_embeddings[transfer] = pooled_embedding\n            \n            \n        elif (not mutual):\n            new_embeddings[transfer] = average_embedding\n                    \n    return new_embeddings\n                            \ndef get_pooled_embedding(neighbors, embeddings, gamma):\n    \"\"\"\n    CPU version of pooled embedding calculation\n    \"\"\"\n    if len(neighbors) == 0:\n        # Return zeros if no neighbors\n        return np.zeros(embeddings.shape[1])\n        \n    embeddings_to_pool = embeddings[neighbors, :]\n    most_recent_embeddings_to_pool = embeddings_to_pool[-min(gamma, embeddings_to_pool.shape[0]):, :]\n    \n    pooled_embedding = most_recent_embeddings_to_pool.mean(axis=0)\n    \n    return pooled_embedding\n\n# Assume EpochLogger implementation is needed\nclass EpochLogger:\n    \"\"\"\n    Callback to log epoch progress\n    \"\"\"\n    def __init__(self):\n        self.epoch = 0\n        \n    def on_epoch_end(self, model):\n        self.epoch += 1\n        print(f\"Completed epoch {self.epoch}\")","metadata":{"_uuid":"217274e4-c3b2-4f1f-a551-d7ce1e57f174","_cell_guid":"b9d77ceb-3d55-4a5e-bcc8-0114d57a1486","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:54:45.654510Z","iopub.execute_input":"2025-03-16T17:54:45.655082Z","iopub.status.idle":"2025-03-16T17:54:45.672854Z","shell.execute_reply.started":"2025-03-16T17:54:45.655057Z","shell.execute_reply":"2025-03-16T17:54:45.671995Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm import tqdm\n\ndef create_network(X_train, y_train, use_gpu=True, batch_size=10000, verbose=True):\n    \"\"\"\n    GPU-accelerated function to create a network structure optimized for fraud detection.\n    \n    Parameters\n    ----------\n    X_train : pandas.DataFrame\n        DataFrame containing transaction features\n    y_train : pandas.Series\n        Series containing fraud labels (1 for fraud, 0 for non-fraud)\n    use_gpu : bool, default=True\n        Whether to use GPU acceleration\n    batch_size : int, default=10000\n        Batch size for GPU operations\n    verbose : bool, default=True\n        Whether to show progress bars\n        \n    Returns\n    -------\n    G : networkx.Graph\n        Graph with nodes representing transactions, customers, merchants, and an artificial fraud node\n    \"\"\"\n    # Check if GPU is available when requested\n    if use_gpu:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        if verbose and device.type == 'cuda':\n            print(f\"Using GPU for network creation: {torch.cuda.get_device_name(0)}\")\n        elif verbose:\n            print(\"GPU requested but not available. Using CPU instead.\")\n    else:\n        device = torch.device('cpu')\n        if verbose:\n            print(\"Using CPU as requested.\")\n    \n    # Create graph\n    G = nx.Graph()\n    \n    # Generate IDs\n    transaction_ids = [f\"txn_{i}\" for i in range(len(X_train))]\n    customer_ids = [f\"cust_{str(cid)}\" for cid in X_train['customerId']]\n    merchant_ids = [f\"merch_{name}_{country}\" for name, country in \n                  zip(X_train['merchantName'], X_train['merchantCountryCode'])]\n    \n    if verbose:\n        print(\"Adding transaction nodes...\")\n    \n    # Add nodes with attributes\n    for i, txn_id in enumerate(tqdm(transaction_ids) if verbose else transaction_ids):\n        # Add transaction node with relevant features\n        G.add_node(txn_id, \n                  type='transaction',\n                  amount_zscore=X_train.iloc[i]['amount_zscore'],\n                  amount_to_avg_ratio=X_train.iloc[i]['amount_to_avg_ratio'],\n                  is_foreign=X_train.iloc[i]['is_foreign_transaction'],\n                  cvv_match=X_train.iloc[i]['cvv_match'],\n                  exp_date_match=X_train.iloc[i]['exp_date_match'])\n    \n    if verbose:\n        print(\"Adding customer and merchant nodes...\")\n    \n    # Add customer and merchant nodes\n    G.add_nodes_from(set(customer_ids), type='customer')\n    G.add_nodes_from(set(merchant_ids), type='merchant')\n    \n    if verbose:\n        print(\"Creating transaction-entity edges...\")\n    \n    # Create edges between transactions and entities\n    for i, txn_id in enumerate(tqdm(transaction_ids) if verbose else transaction_ids):\n        # Connect transaction to customer\n        G.add_edge(txn_id, customer_ids[i], edge_type='customer_transaction')\n        \n        # Connect transaction to merchant\n        G.add_edge(txn_id, merchant_ids[i], edge_type='merchant_transaction')\n    \n    if verbose:\n        print(\"Processing merchant proximity...\")\n    \n    # Extract merchant coordinates\n    merchant_df = pd.DataFrame({\n        'merchant_id': merchant_ids,\n        'lat': X_train['merchant_lat'].values,\n        'lon': X_train['merchant_lon'].values\n    }).drop_duplicates('merchant_id')\n    \n    # Filter out merchants with invalid coordinates\n    valid_merchant_df = merchant_df.dropna()\n    \n    # GPU-accelerated geographical proximity calculation\n    if len(valid_merchant_df) > 0:\n        if use_gpu and device.type == 'cuda':\n            # Use GPU for proximity calculation\n            coords = torch.tensor(valid_merchant_df[['lat', 'lon']].values, device=device, dtype=torch.float32)\n            \n            # Process in batches if dataset is large\n            merchant_edges = []\n            n_merchants = len(valid_merchant_df)\n            \n            for i in range(0, n_merchants, batch_size):\n                end_idx = min(i + batch_size, n_merchants)\n                batch_coords = coords[i:end_idx]\n                \n                # Calculate pairwise distances using GPU\n                # ||a - b||^2 = ||a||^2 + ||b||^2 - 2*a*b\n                a_norm = torch.sum(batch_coords**2, dim=1).view(-1, 1)\n                b_norm = torch.sum(coords**2, dim=1).view(1, -1)\n                dist_matrix = a_norm + b_norm - 2 * torch.mm(batch_coords, coords.t())\n                dist_matrix = torch.sqrt(torch.clamp(dist_matrix, min=0))\n                \n                # Find close merchants\n                close_pairs = torch.nonzero(dist_matrix < 0.01, as_tuple=False)\n                \n                # Add valid pairs to edges list\n                for pair in close_pairs:\n                    idx1, idx2 = pair[0].item() + i, pair[1].item()\n                    if idx1 != idx2:  # Avoid self-loops\n                        dist = dist_matrix[pair[0], pair[1]].item()\n                        m1 = valid_merchant_df.iloc[idx1]['merchant_id']\n                        m2 = valid_merchant_df.iloc[idx2]['merchant_id']\n                        merchant_edges.append((m1, m2, {'edge_type': 'location_proximity', 'weight': 1-dist*100}))\n            \n            # Add all edges to graph\n            G.add_edges_from(merchant_edges)\n        \n        else:\n            # CPU-based implementation using scikit-learn's NearestNeighbors\n            coords = valid_merchant_df[['lat', 'lon']].values\n            \n            # Use NearestNeighbors for efficient proximity search\n            nbrs = NearestNeighbors(radius=0.01, algorithm='ball_tree').fit(coords)\n            distances, indices = nbrs.radius_neighbors(coords)\n            \n            # Create edges for close merchants\n            merchant_edges = []\n            for i, idx_list in enumerate(indices):\n                for j, dist in zip(idx_list, distances[i]):\n                    if i != j:  # Avoid self-loops\n                        m1 = valid_merchant_df.iloc[i]['merchant_id']\n                        m2 = valid_merchant_df.iloc[j]['merchant_id']\n                        merchant_edges.append((m1, m2, {'edge_type': 'location_proximity', 'weight': 1-dist*100}))\n            \n            # Add all edges to graph\n            G.add_edges_from(merchant_edges)\n    \n    if verbose:\n        print(\"Processing sequential transactions...\")\n    \n    # Create a mapping from customer to their transactions\n    customer_txn_map = {}\n    for i, cust_id in enumerate(customer_ids):\n        if cust_id not in customer_txn_map:\n            customer_txn_map[cust_id] = []\n        customer_txn_map[cust_id].append((transaction_ids[i], X_train.iloc[i]['timeDelta']))\n    \n    # Connect transactions from the same customer if they occurred within a short time window\n    sequential_edges = []\n    for cust_id, txns in customer_txn_map.items():\n        if len(txns) > 1:\n            # Sort transactions by time\n            txns.sort(key=lambda x: x[1])\n            \n            # Connect sequential transactions within 24 hours\n            for i in range(len(txns) - 1):\n                for j in range(i + 1, len(txns)):\n                    time_delta = abs(txns[j][1] - txns[i][1])\n                    if time_delta < 86400:  # Within 24 hours (in seconds)\n                        sequential_edges.append((txns[i][0], txns[j][0], {\n                            'edge_type': 'sequential_transactions',\n                            'time_diff': time_delta\n                        }))\n    \n    # Add all sequential edges to graph\n    G.add_edges_from(sequential_edges)\n    \n    if verbose:\n        print(\"Adding fraud node connections...\")\n    \n    # Add artificial fraud node and connect it to all fraudulent transactions\n    fraud_node = \"ARTIFICIAL_FRAUD_NODE\"\n    G.add_node(fraud_node, type='artificial_fraud')\n    \n    # Connect to fraudulent transactions\n    fraud_edges = [(txn_id, fraud_node, {'edge_type': 'is_fraud'}) \n                  for i, txn_id in enumerate(transaction_ids) if y_train.iloc[i] == 1]\n    G.add_edges_from(fraud_edges)\n    \n    if verbose:\n        print(f\"Network creation complete. Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n    \n    return G\n\n# Optional: A utility function to estimate memory requirements\ndef estimate_network_memory(X_train, y_train):\n    \"\"\"\n    Estimates the memory requirements for network creation\n    \"\"\"\n    n_transactions = len(X_train)\n    n_customers = X_train['customerId'].nunique()\n    n_merchants = X_train[['merchantName', 'merchantCountryCode']].drop_duplicates().shape[0]\n    \n    # Estimate nodes (transactions, customers, merchants, fraud node)\n    n_nodes = n_transactions + n_customers + n_merchants + 1\n    \n    # Estimate edges (transaction-customer, transaction-merchant, merchant proximity, sequential, fraud)\n    n_edges_base = n_transactions * 2  # Each transaction connects to a customer and merchant\n    n_fraud_edges = y_train.sum()\n    \n    # Rough estimate of merchant proximity edges (assuming 5% of merchants are close)\n    n_merchant_proximity = int(n_merchants * n_merchants * 0.05)\n    \n    # Rough estimate of sequential transaction edges (assuming 10% of transactions per customer are sequential)\n    avg_txn_per_customer = n_transactions / n_customers\n    n_sequential = int(n_customers * (avg_txn_per_customer * (avg_txn_per_customer - 1) / 2) * 0.1)\n    \n    total_edges = n_edges_base + n_fraud_edges + n_merchant_proximity + n_sequential\n    \n    # Estimate memory (rough approximation)\n    memory_per_node = 100  # bytes\n    memory_per_edge = 60   # bytes\n    \n    estimated_memory = (n_nodes * memory_per_node + total_edges * memory_per_edge) / (1024 * 1024)  # in MB\n    \n    return {\n        'n_nodes': n_nodes,\n        'n_edges': total_edges,\n        'estimated_memory_mb': estimated_memory,\n        'recommended_gpu_memory_gb': max(2, int(estimated_memory / 1024 * 3))  # 3x buffer\n    }","metadata":{"_uuid":"f26d7c72-6eda-48c3-bbc8-c3f648904ea6","_cell_guid":"4282c00a-d7c5-4c9b-ad77-71be760a7929","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:54:45.673922Z","iopub.execute_input":"2025-03-16T17:54:45.674169Z","iopub.status.idle":"2025-03-16T17:54:45.751940Z","shell.execute_reply.started":"2025-03-16T17:54:45.674149Z","shell.execute_reply":"2025-03-16T17:54:45.751391Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from gensim.models.callbacks import CallbackAny2Vec\n\ndef check_edgelist(edgelist):\n\n    if not isinstance(edgelist, list):\n        edgelist = list(edgelist)\n    \n\nclass EpochLogger(CallbackAny2Vec):\n    '''Callback to log information about training'''\n\n    def __init__(self):\n        self.epoch = 0\n\n    def on_epoch_begin(self, model):\n        print(\"Epoch #{} start\".format(self.epoch))\n\n    def on_epoch_end(self, model):\n        print(\"Epoch #{} end\".format(self.epoch))\n        self.epoch += 1","metadata":{"_uuid":"73f50e47-8cdc-42f4-ae9c-2742651113ac","_cell_guid":"a776b68f-cbc7-4c0e-8005-638adbe50d88","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:54:45.752666Z","iopub.execute_input":"2025-03-16T17:54:45.753137Z","iopub.status.idle":"2025-03-16T17:54:45.758097Z","shell.execute_reply.started":"2025-03-16T17:54:45.753115Z","shell.execute_reply":"2025-03-16T17:54:45.757128Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class InductiveDeepwalk(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Implementation of DeepWalk with inductive capabilities for fraud detection.\n    GPU-accelerated version.\n    \n    Parameters\n    ----------\n    dimensions : int\n        Number of dimensions in the embeddings\n    walk_len : int\n        Length of each random walk\n    walk_num : int\n        Number of random walks per node\n    epochs : int, default=5\n        Number of training epochs\n    workers : int, default=1\n        Number of parallel workers\n    window_size : int, default=5\n        Context window size for Word2Vec\n    verbose : int, default=0\n        Verbosity level\n    use_gpu : bool, default=True\n        Whether to use GPU acceleration\n    \"\"\"\n    def __init__(self, dimensions, walk_len, walk_num, epochs=5, workers=1, window_size=5, verbose=0, use_gpu=True):\n        self.dimensions = dimensions\n        self.walk_len = walk_len\n        self.walk_num = walk_num\n        self.epochs = epochs\n        self.workers = workers\n        self.window_size = window_size\n        self.first_fit = True\n        self.verbose = verbose\n        self.use_gpu = use_gpu\n        \n        # Check if GPU is available\n        if self.use_gpu:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            if self.verbose > 0:\n                print(f\"Using device: {self.device}\")\n            if self.device.type == 'cpu' and self.use_gpu:\n                print(\"Warning: GPU requested but not available. Using CPU instead.\")\n        else:\n            self.device = torch.device('cpu')\n            if self.verbose > 0:\n                print(\"Using CPU as requested.\")\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model with X.\n        \n        Parameters\n        ----------\n        X : pandas.DataFrame\n            Training data\n        y : pandas.Series\n            Target values (fraud labels)\n            \n        Returns\n        -------\n        self : object\n            Returns self\n        \"\"\"\n        if self.verbose > 0:\n            print(\"Parsing input into network format.\")\n        \n        # Create network using the updated function\n        self.G = create_network(X, y)\n        \n        # Get transaction nodes\n        transaction_nodes = [n for n, d in self.G.nodes(data=True) if d.get('type') == 'transaction']\n        \n        # Extract transaction IDs as integers for proper indexing\n        self.transaction_ids = [n.split('_')[1] for n in transaction_nodes]\n        \n        callbacks = []\n        if self.verbose > 0:\n            print(\"Running network representation algorithm.\")\n            epochlogger = EpochLogger()\n            callbacks = [epochlogger]\n        \n        # Configure Word2Vec parameters with GPU support if available\n        w2v_params = {\n            'workers': self.workers, \n            'window': self.window_size, \n            'callbacks': callbacks,\n            'compute_loss': True\n        }\n        \n        # Train Node2Vec model\n        g2v = Node2Vec(\n            n_components=self.dimensions,\n            walklen=self.walk_len,\n            epochs=self.walk_num,\n            verbose=self.verbose,\n            w2vparams=w2v_params\n        )\n        g2v.fit(self.G)\n        self.model = g2v.model\n        \n        # Create dictionary of node embeddings\n        self.node_embeddings = {}\n        for node in self.G.nodes():\n            try:\n                self.node_embeddings[node] = self.model.wv[node]\n            except KeyError:\n                # Handle nodes not in vocabulary\n                self.node_embeddings[node] = np.zeros(self.dimensions)\n        \n        # Create array of transaction embeddings for easier access\n        self.embeddings = np.zeros((len(transaction_nodes), self.dimensions))\n        for i, txn_id in enumerate(transaction_nodes):\n            self.embeddings[i] = self.node_embeddings[txn_id]\n            \n        # Convert embeddings to PyTorch tensors for GPU processing\n        if self.use_gpu and self.device.type == 'cuda':\n            self.embeddings_tensor = torch.tensor(self.embeddings, device=self.device)\n        \n        self.is_fitted_ = True\n        self.first_fit = True\n        return self\n    \n    def transform(self, X):\n        \"\"\"\n        Transform X.\n        \n        Parameters\n        ----------\n        X : pandas.DataFrame\n            Test data containing transaction features\n            \n        Returns\n        -------\n        np.ndarray\n            Transaction embeddings\n        \"\"\"\n        check_is_fitted(self, 'is_fitted_')\n        \n        if self.first_fit:\n            if self.verbose > 0:\n                print(\"Retrieving embeddings for training data.\")\n            results = self.embeddings\n            self.first_fit = False\n        else:\n            if self.verbose > 0:\n                print(\"Running inductive pooling extension.\")\n            results = inductive_pooling(X, self.embeddings, self.G, workers=self.workers, \n                                       use_gpu=self.use_gpu, device=self.device)\n        \n        return results","metadata":{"_uuid":"150e5567-445a-468b-afde-102c14bf0cc3","_cell_guid":"1d0cd7f1-a0aa-4e1e-b4a9-7a26381326a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:54:45.758933Z","iopub.execute_input":"2025-03-16T17:54:45.759133Z","iopub.status.idle":"2025-03-16T17:54:45.784814Z","shell.execute_reply.started":"2025-03-16T17:54:45.759117Z","shell.execute_reply":"2025-03-16T17:54:45.784257Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom geopy.distance import geodesic\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"b1fc45ff-b955-4912-8204-62d06f6f7a04","_cell_guid":"5a302ffb-991e-4bb2-b544-0c9b90bffddb","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:54:45.786109Z","iopub.execute_input":"2025-03-16T17:54:45.786347Z","iopub.status.idle":"2025-03-16T17:54:46.348894Z","shell.execute_reply.started":"2025-03-16T17:54:45.786328Z","shell.execute_reply":"2025-03-16T17:54:46.348211Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Parameters\ndimensions = 32\nwalk_len = 80\nwalk_num = 10\nwindow_size = 5\n# the 'workers' parameter is used for multi-processing.\nworkers = 4","metadata":{"_uuid":"84b03b20-4ad1-4e65-bb9c-272027ad3a47","_cell_guid":"60b513e1-41d7-450f-bd5b-485efeaa3230","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:54:48.347759Z","iopub.execute_input":"2025-03-16T17:54:48.348419Z","iopub.status.idle":"2025-03-16T17:54:48.352331Z","shell.execute_reply.started":"2025-03-16T17:54:48.348387Z","shell.execute_reply":"2025-03-16T17:54:48.351541Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Load Data","metadata":{"_uuid":"2ef7befb-daec-43dc-84b1-25aaab5dce7c","_cell_guid":"d5714981-f9ef-4447-9ec6-88390f2f2f40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\n### PATH TO DEMO DATA ###\ndemo_data_path = '/kaggle/input/latlondata/output.csv'\n\ndf = pd.read_csv(demo_data_path)\n\n# Convert date columns to datetime format\ndate_columns = ['transactionDateTime', 'currentExpDate', 'accountOpenDate', 'dateOfLastAddressChange']\nfor col in date_columns:\n    df[col] = pd.to_datetime(df[col], errors='coerce')","metadata":{"_uuid":"de45ead8-d2fd-4a0e-b2b4-333166cb7598","_cell_guid":"8b6a03d4-31b9-45a9-9d75-f940a22cd03c","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:26:46.290827Z","iopub.execute_input":"2025-03-16T17:26:46.291128Z","iopub.status.idle":"2025-03-16T17:26:49.568077Z","shell.execute_reply.started":"2025-03-16T17:26:46.291100Z","shell.execute_reply":"2025-03-16T17:26:49.567420Z"},"scrolled":true},"outputs":[],"execution_count":51},{"cell_type":"code","source":"df.dropna(subset=['merchant_lat'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:26:49.569062Z","iopub.execute_input":"2025-03-16T17:26:49.569400Z","iopub.status.idle":"2025-03-16T17:26:49.653263Z","shell.execute_reply.started":"2025-03-16T17:26:49.569368Z","shell.execute_reply":"2025-03-16T17:26:49.652567Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:26:49.654090Z","iopub.execute_input":"2025-03-16T17:26:49.654340Z","iopub.status.idle":"2025-03-16T17:26:49.659216Z","shell.execute_reply.started":"2025-03-16T17:26:49.654320Z","shell.execute_reply":"2025-03-16T17:26:49.658564Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"(303005, 33)"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"# -------------- Feature Engineering --------------\n\n# 1. Velocity of Transactions for Each User\nprint(\"Engineering transaction velocity features...\")\n# Sort transactions by customer and datetime\ndf = df.sort_values(['customerId', 'transactionDateTime'])\n\n# Calculate time difference between consecutive transactions per customer\ndf['prevTransactionTime'] = df.groupby('customerId')['transactionDateTime'].shift(1)\ndf['timeDelta'] = (df['transactionDateTime'] - df['prevTransactionTime']).dt.total_seconds() / 3600  # in hours\n\n# Count transactions in the last 24 hours, 7 days\ndef count_transactions_in_timeframe(group, hours):\n    result = []\n    for i, row in group.iterrows():\n        current_time = row['transactionDateTime']\n        timeframe_start = current_time - pd.Timedelta(hours=hours)\n        count = len(group[(group['transactionDateTime'] > timeframe_start) &\n                          (group['transactionDateTime'] < current_time)])\n        result.append(count)\n    return result\n\n# Apply counting functions to each customer group\ncustomer_groups = df.groupby('customerId')\ndf['txn_count_24h'] = customer_groups.apply(lambda x: count_transactions_in_timeframe(x, 24)).explode().values\ndf['txn_count_7d'] = customer_groups.apply(lambda x: count_transactions_in_timeframe(x, 168)).explode().values  # 7*24=168","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:26:49.660043Z","iopub.execute_input":"2025-03-16T17:26:49.660319Z","iopub.status.idle":"2025-03-16T17:35:43.351291Z","shell.execute_reply.started":"2025-03-16T17:26:49.660299Z","shell.execute_reply":"2025-03-16T17:35:43.350275Z"}},"outputs":[{"name":"stdout","text":"Engineering transaction velocity features...\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# 2. Unusual Spending Spikes\nprint(\"Engineering spending pattern features...\")\n# Calculate average transaction amount per customer\ncustomer_avg_amount = df.groupby('customerId')['transactionAmount'].transform('mean')\ncustomer_std_amount = df.groupby('customerId')['transactionAmount'].transform('std')\n\n# Calculate z-score of transaction amount\ndf['amount_zscore'] = (df['transactionAmount'] - customer_avg_amount) / customer_std_amount.replace(0, 1)\n\n# Calculate ratio of current transaction to average\ndf['amount_to_avg_ratio'] = df['transactionAmount'] / customer_avg_amount.replace(0, 1)\n\n# Calculate cumulative amount spent in last 24 hours\ndef sum_amount_in_timeframe(group, hours):\n    result = []\n    for i, row in group.iterrows():\n        current_time = row['transactionDateTime']\n        timeframe_start = current_time - pd.Timedelta(hours=hours)\n        amount_sum = group[(group['transactionDateTime'] > timeframe_start) &\n                           (group['transactionDateTime'] < current_time)]['transactionAmount'].sum()\n        result.append(amount_sum)\n    return result\n\ndf['amount_24h'] = customer_groups.apply(lambda x: sum_amount_in_timeframe(x, 24)).explode().values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:35:43.352144Z","iopub.execute_input":"2025-03-16T17:35:43.352417Z","iopub.status.idle":"2025-03-16T17:40:47.385805Z","shell.execute_reply.started":"2025-03-16T17:35:43.352395Z","shell.execute_reply":"2025-03-16T17:40:47.384931Z"}},"outputs":[{"name":"stdout","text":"Engineering spending pattern features...\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Fill previous valid latitude and longitude recursively using ffill()\ndf['prev_lat'] = df.groupby('customerId')['merchant_lat'].ffill().shift(1)\ndf['prev_lon'] = df.groupby('customerId')['merchant_lon'].ffill().shift(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:40:47.386583Z","iopub.execute_input":"2025-03-16T17:40:47.386790Z","iopub.status.idle":"2025-03-16T17:40:47.404483Z","shell.execute_reply.started":"2025-03-16T17:40:47.386773Z","shell.execute_reply":"2025-03-16T17:40:47.403531Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Calculate distance in kilometers\ndef calculate_distance(row):\n    if pd.isna(row['prev_lat']) or pd.isna(row['prev_lon']) or pd.isna(row['merchant_lat']) or pd.isna(row['merchant_lon']):\n        return 0\n    return geodesic((row['prev_lat'], row['prev_lon']), (row['merchant_lat'], row['merchant_lon'])).kilometers\n\ndf['distance_from_prev_txn'] = df.apply(calculate_distance, axis=1)\n\n# Calculate speed (km/h) - distance divided by time difference\ndf['speed_kmph'] = np.where(df['timeDelta'] > 0, df['distance_from_prev_txn'] / df['timeDelta'], 0)\n\n# Calculate if transaction is in a different country from previous\ndf['prev_country'] = df.groupby('customerId')['merchantCountryCode'].shift(1)\ndf['different_country'] = (df['merchantCountryCode'] != df['prev_country']).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:40:47.405281Z","iopub.execute_input":"2025-03-16T17:40:47.405569Z","iopub.status.idle":"2025-03-16T17:41:54.915434Z","shell.execute_reply.started":"2025-03-16T17:40:47.405542Z","shell.execute_reply":"2025-03-16T17:41:54.914501Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# 4. Additional Features\nprint(\"Engineering additional features...\")\n# Binary flags\ndf['cvv_match'] = (df['cardCVV'] == df['enteredCVV']).astype(int)\ndf['exp_date_match'] = df['expirationDateKeyInMatch'].astype(int)\ndf['is_foreign_transaction'] = (df['acqCountry'] != df['merchantCountryCode']).astype(int)\n\n# Calculate the ratio of transaction amount to credit limit\ndf['amount_to_limit_ratio'] = df['transactionAmount'] / df['creditLimit'].replace(0, 1)\n\n# Calculate the ratio of transaction amount to available money\ndf['amount_to_available_ratio'] = df['transactionAmount'] / df['availableMoney'].replace(0, 1)\n\n# Calculate days since account opening\ndf['account_age_days'] = (df['transactionDateTime'] - df['accountOpenDate']).dt.days\n\n# Calculate days since last address change\ndf['days_since_address_change'] = (df['transactionDateTime'] - df['dateOfLastAddressChange']).dt.days\n\ndf['isOnline'] = np.where(df['merchantCategoryCode'] == 'online_retail', 1, 0)\n# One-hot encode categorical variables\ncategorical_cols = ['posEntryMode', 'posConditionCode', 'merchantCategoryCode', 'transactionType']\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:41:54.916403Z","iopub.execute_input":"2025-03-16T17:41:54.916682Z","iopub.status.idle":"2025-03-16T17:41:55.393547Z","shell.execute_reply.started":"2025-03-16T17:41:54.916648Z","shell.execute_reply":"2025-03-16T17:41:55.392805Z"}},"outputs":[{"name":"stdout","text":"Engineering additional features...\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:41:55.394344Z","iopub.execute_input":"2025-03-16T17:41:55.394561Z","iopub.status.idle":"2025-03-16T17:41:55.399743Z","shell.execute_reply.started":"2025-03-16T17:41:55.394542Z","shell.execute_reply":"2025-03-16T17:41:55.399114Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'accountNumber', 'customerId', 'creditLimit',\n       'availableMoney', 'transactionDateTime', 'transactionAmount',\n       'merchantName', 'acqCountry', 'merchantCountryCode', 'posEntryMode',\n       'posConditionCode', 'merchantCategoryCode', 'currentExpDate',\n       'accountOpenDate', 'dateOfLastAddressChange', 'cardCVV', 'enteredCVV',\n       'cardLast4Digits', 'transactionType', 'echoBuffer', 'currentBalance',\n       'merchantCity', 'merchantState', 'merchantZip', 'cardPresent',\n       'posOnPremises', 'recurringAuthInd', 'expirationDateKeyInMatch',\n       'isFraud', 'merchant_loc', 'merchant_lat', 'merchant_lon',\n       'prevTransactionTime', 'timeDelta', 'txn_count_24h', 'txn_count_7d',\n       'amount_zscore', 'amount_to_avg_ratio', 'amount_24h', 'prev_lat',\n       'prev_lon', 'distance_from_prev_txn', 'speed_kmph', 'prev_country',\n       'different_country', 'cvv_match', 'exp_date_match',\n       'is_foreign_transaction', 'amount_to_limit_ratio',\n       'amount_to_available_ratio', 'account_age_days',\n       'days_since_address_change', 'isOnline'],\n      dtype='object')"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"categorical_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:41:55.400452Z","iopub.execute_input":"2025-03-16T17:41:55.400722Z","iopub.status.idle":"2025-03-16T17:41:55.416033Z","shell.execute_reply.started":"2025-03-16T17:41:55.400696Z","shell.execute_reply":"2025-03-16T17:41:55.415457Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"['posEntryMode', 'posConditionCode', 'merchantCategoryCode', 'transactionType']"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"\nprint(\"GPU-accelerated feature engineering complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:41:55.416841Z","iopub.execute_input":"2025-03-16T17:41:55.417085Z","iopub.status.idle":"2025-03-16T17:41:55.431447Z","shell.execute_reply.started":"2025-03-16T17:41:55.417058Z","shell.execute_reply":"2025-03-16T17:41:55.430751Z"}},"outputs":[{"name":"stdout","text":"GPU-accelerated feature engineering complete!\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"df = df[[col for col in df.columns if col != 'isFraud'] + ['isFraud']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:41:55.432217Z","iopub.execute_input":"2025-03-16T17:41:55.432481Z","iopub.status.idle":"2025-03-16T17:41:55.510687Z","shell.execute_reply.started":"2025-03-16T17:41:55.432462Z","shell.execute_reply":"2025-03-16T17:41:55.509757Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"df.to_csv('NewDatFrame.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:44:20.603241Z","iopub.execute_input":"2025-03-16T17:44:20.603599Z","iopub.status.idle":"2025-03-16T17:44:30.679366Z","shell.execute_reply.started":"2025-03-16T17:44:20.603574Z","shell.execute_reply":"2025-03-16T17:44:30.678377Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n# Split into train and test set\ndf_train = pd.read_csv('/kaggle/input/latestdata/NewDatFrame.csv')","metadata":{"_uuid":"35b92cb1-b33a-4cac-9950-dbb5f386b7dc","_cell_guid":"ff5d7f5c-8896-4a61-a6f2-49d390625626","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:55:11.314801Z","iopub.execute_input":"2025-03-16T17:55:11.315120Z","iopub.status.idle":"2025-03-16T17:55:14.823019Z","shell.execute_reply.started":"2025-03-16T17:55:11.315096Z","shell.execute_reply":"2025-03-16T17:55:14.822286Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"X_train=df_train.iloc[:, :-1]\ny_train=df_train.iloc[:, -1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:55:18.316731Z","iopub.execute_input":"2025-03-16T17:55:18.317067Z","iopub.status.idle":"2025-03-16T17:55:18.386590Z","shell.execute_reply.started":"2025-03-16T17:55:18.317041Z","shell.execute_reply":"2025-03-16T17:55:18.385857Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print('Converted Training Data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:55:18.554225Z","iopub.execute_input":"2025-03-16T17:55:18.554516Z","iopub.status.idle":"2025-03-16T17:55:18.558968Z","shell.execute_reply.started":"2025-03-16T17:55:18.554490Z","shell.execute_reply":"2025-03-16T17:55:18.558164Z"}},"outputs":[{"name":"stdout","text":"Converted Training Data\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# %load /usr/local/lib/python3.10/dist-packages/nodevectors/node2vec.py\nimport numba\nimport numpy as np\nimport pandas as pd\nimport time\nimport warnings\n\n# Gensim triggers automatic useless warnings for windows users...\nwarnings.simplefilter(\"ignore\", category=UserWarning)\nimport gensim\nwarnings.simplefilter(\"default\", category=UserWarning)\n\n\nimport csrgraph as cg\nfrom nodevectors.embedders import BaseNodeEmbedder\n\nclass Node2Vec(BaseNodeEmbedder):\n    def __init__(\n        self, \n        n_components=32,\n        walklen=30, \n        epochs=20,\n        return_weight=1.,\n        neighbor_weight=1.,\n        threads=0, \n        keep_walks=False,\n        verbose=True,\n        w2vparams={\"window\":10, \"negative\":5, \"iter\":10,\n                   \"batch_words\":128}):\n        \"\"\"\n        Parameters\n        ----------\n        walklen : int\n            length of the random walks\n        epochs : int\n            number of times to start a walk from each nodes\n        threads : int\n            number of threads to use. 0 is full use\n        n_components : int\n            number of resulting dimensions for the embedding\n            This should be set here rather than in the w2vparams arguments\n        return_weight : float in (0, inf]\n            Weight on the probability of returning to node coming from\n            Having this higher tends the walks to be \n            more like a Breadth-First Search.\n            Having this very high  (> 2) makes search very local.\n            Equal to the inverse of p in the Node2Vec paper.\n        neighbor_weight : float in (0, inf]\n            Weight on the probability of visitng a neighbor node\n            to the one we're coming from in the random walk\n            Having this higher tends the walks to be \n            more like a Depth-First Search.\n            Having this very high makes search more outward.\n            Having this very low makes search very local.\n            Equal to the inverse of q in the Node2Vec paper.\n        keep_walks : bool\n            Whether to save the random walks in the model object after training\n        w2vparams : dict\n            dictionary of parameters to pass to gensim's word2vec\n            Don't set the embedding dimensions through arguments here.\n        \"\"\"\n        if type(threads) is not int:\n            raise ValueError(\"Threads argument must be an int!\")\n        if walklen < 1 or epochs < 1:\n            raise ValueError(\"Walklen and epochs arguments must be > 1\")\n        self.n_components = n_components\n        self.walklen = walklen\n        self.epochs = epochs\n        self.keep_walks = keep_walks\n        if 'size' in w2vparams.keys():\n            raise AttributeError(\"Embedding dimensions should not be set \"\n                + \"through w2v parameters, but through n_components\")\n        self.w2vparams = w2vparams\n        self.return_weight = return_weight\n        self.neighbor_weight = neighbor_weight\n        if threads == 0:\n            threads = numba.config.NUMBA_DEFAULT_NUM_THREADS\n        self.threads = threads\n        w2vparams['workers'] = threads\n        self.verbose = verbose\n\n    def fit(self, G):\n        \"\"\"\n        NOTE: Currently only support str or int as node name for graph\n        Parameters\n        ----------\n        G : graph data\n            Graph to embed\n            Can be any graph type that's supported by csrgraph library\n            (NetworkX, numpy 2d array, scipy CSR matrix, CSR matrix components)\n        \"\"\"\n        if not isinstance(G, cg.csrgraph):\n            G = cg.csrgraph(G, threads=self.threads)\n        if G.threads != self.threads:\n            G.set_threads(self.threads)\n        # Because networkx graphs are actually iterables of their nodes\n        #   we do list(G) to avoid networkx 1.X vs 2.X errors\n        node_names = G.names\n        if type(node_names[0]) not in [int, str, np.int32, np.uint32, \n                                       np.int64, np.uint64]:\n            raise ValueError(\"Graph node names must be int or str!\")\n        # Adjacency matrix\n        walks_t = time.time()\n        if self.verbose:\n            print(\"Making walks...\", end=\" \")\n        self.walks = G.random_walks(walklen=self.walklen, \n                                    epochs=self.epochs,\n                                    return_weight=self.return_weight,\n                                    neighbor_weight=self.neighbor_weight)\n        if self.verbose:\n            print(f\"Done, T={time.time() - walks_t:.2f}\")\n            print(\"Mapping Walk Names...\", end=\" \")\n        map_t = time.time()\n        self.walks = pd.DataFrame(self.walks)\n        # Map nodeId -> node name\n        node_dict = dict(zip(np.arange(len(node_names)), node_names))\n        for col in self.walks.columns:\n            self.walks[col] = self.walks[col].map(node_dict).astype(str)\n        # Somehow gensim only trains on this list iterator\n        # it silently mistrains on array input\n        self.walks = [list(x) for x in self.walks.itertuples(False, None)]\n        if self.verbose:\n            print(f\"Done, T={time.time() - map_t:.2f}\")\n            print(\"Training W2V...\", end=\" \")\n            if gensim.models.word2vec.FAST_VERSION < 1:\n                print(\"WARNING: gensim word2vec version is unoptimized\"\n                    \"Try version 3.6 if on windows, versions 3.7 \"\n                    \"and 3.8 have had issues\")\n        w2v_t = time.time()\n        # Train gensim word2vec model on random walks\n        self.model = gensim.models.Word2Vec(\n            sentences=self.walks,\n            vector_size=self.n_components,\n            **self.w2vparams)\n        if not self.keep_walks:\n            del self.walks\n        if self.verbose:\n            print(f\"Done, T={time.time() - w2v_t:.2f}\")\n\n    def fit_transform(self, G):\n        \"\"\"\n        NOTE: Currently only support str or int as node name for graph\n        Parameters\n        ----------\n        G : graph data\n            Graph to embed\n            Can be any graph type that's supported by csrgraph library\n            (NetworkX, numpy 2d array, scipy CSR matrix, CSR matrix components)\n        \"\"\"\n        if not isinstance(G, cg.csrgraph):\n            G = cg.csrgraph(G, threads=self.threads)\n        self.fit(G)\n        w = np.array(\n            pd.DataFrame.from_records(\n            pd.Series(np.arange(len(G.nodes())))\n              .apply(self.predict)\n              .values)\n        )\n        return w\n    \n    def predict(self, node_name):\n        \"\"\"\n        Return vector associated with node\n        node_name : str or int\n            either the node ID or node name depending on graph format\n        \"\"\"\n        # current hack to work around word2vec problem\n        # ints need to be str -_-\n        if type(node_name) is not str:\n            node_name = str(node_name)\n        return self.model.wv.__getitem__(node_name)\n\n    def save_vectors(self, out_file):\n        \"\"\"\n        Save as embeddings in gensim.models.KeyedVectors format\n        \"\"\"\n        self.model.wv.save_word2vec_format(out_file)\n\n    def load_vectors(self, out_file):\n        \"\"\"\n        Load embeddings from gensim.models.KeyedVectors format\n        \"\"\"\n        self.model = gensim.wv.load_word2vec_format(out_file)","metadata":{"_uuid":"039ae2dd-550e-4a85-9d0b-df1be64605ae","_cell_guid":"b72e6246-1b68-4d47-8ade-de37e7959c7e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:55:20.605905Z","iopub.execute_input":"2025-03-16T17:55:20.606313Z","iopub.status.idle":"2025-03-16T17:55:20.620436Z","shell.execute_reply.started":"2025-03-16T17:55:20.606277Z","shell.execute_reply":"2025-03-16T17:55:20.619578Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import multiprocessing\n\nworkers=multiprocessing.cpu_count()\nembedder = InductiveDeepwalk(dimensions=dimensions, walk_len = walk_len, walk_num=walk_num, workers=workers, verbose=0)","metadata":{"_uuid":"a8288319-2532-4584-9c70-bff1bef3c514","_cell_guid":"8afc8144-e84f-414b-996f-ef3b4f7e664f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:55:23.921996Z","iopub.execute_input":"2025-03-16T17:55:23.922487Z","iopub.status.idle":"2025-03-16T17:55:23.927835Z","shell.execute_reply.started":"2025-03-16T17:55:23.922446Z","shell.execute_reply":"2025-03-16T17:55:23.926830Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print('Starting embedding')\nembedobj=embedder.fit(X_train, y_train)\nprint('ending embedding')","metadata":{"_uuid":"5f2093dc-ea29-434e-93d5-08884db572b5","_cell_guid":"ce6573f8-b323-4ea9-8080-55c998bb3f17","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:55:24.130953Z","iopub.execute_input":"2025-03-16T17:55:24.131344Z"}},"outputs":[{"name":"stdout","text":"Starting embedding\nUsing GPU for network creation: Tesla T4\nAdding transaction nodes...\n","output_type":"stream"},{"name":"stderr","text":"100%|| 303005/303005 [01:34<00:00, 3200.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Adding customer and merchant nodes...\nCreating transaction-entity edges...\n","output_type":"stream"},{"name":"stderr","text":"100%|| 303005/303005 [00:01<00:00, 169661.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing merchant proximity...\nProcessing sequential transactions...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"params={'eval_metric' : ['auc','aucpr', 'logloss'],\n                          'n_estimators':300, \n                          'n_jobs':8, \n                          'learning_rate':0.1, \n                          'seed':42, \n                          'colsample_bytree' : 0.6,\n                          'colsample_bylevel':0.9, \n                          'subsample' : 0.9}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:21:12.988436Z","iopub.execute_input":"2025-03-16T17:21:12.988767Z","iopub.status.idle":"2025-03-16T17:21:12.992658Z","shell.execute_reply.started":"2025-03-16T17:21:12.988731Z","shell.execute_reply":"2025-03-16T17:21:12.991848Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"np.save('embeddings.npy',embedobj.embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('emebdding shape:',embedobj.embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:21:12.993851Z","iopub.execute_input":"2025-03-16T17:21:12.994086Z","iopub.status.idle":"2025-03-16T17:21:13.017672Z","shell.execute_reply.started":"2025-03-16T17:21:12.994067Z","shell.execute_reply":"2025-03-16T17:21:13.016962Z"}},"outputs":[{"name":"stdout","text":"emebdding shape: (3000, 32)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"classifier = xgb.XGBClassifier(**params)","metadata":{"_uuid":"3cebe185-f3c1-45df-a2ff-b62fa9b230c1","_cell_guid":"4a3a7c62-8de5-4699-9be2-9a99e8c0a684","trusted":true,"collapsed":false,"metadata":{},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:22:35.529748Z","iopub.execute_input":"2025-03-16T17:22:35.530137Z","iopub.status.idle":"2025-03-16T17:22:35.533983Z","shell.execute_reply.started":"2025-03-16T17:22:35.530105Z","shell.execute_reply":"2025-03-16T17:22:35.533089Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"classifier.fit(embedobj.embeddings[:300000],y_train[:300000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:22:35.734443Z","iopub.execute_input":"2025-03-16T17:22:35.734675Z","iopub.status.idle":"2025-03-16T17:22:35.891915Z","shell.execute_reply.started":"2025-03-16T17:22:35.734655Z","shell.execute_reply":"2025-03-16T17:22:35.891053Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=0.9, colsample_bynode=None,\n              colsample_bytree=0.6, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=['auc', 'aucpr', 'logloss'],\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=300, n_jobs=8,\n              num_parallel_tree=None, ...)","text/html":"<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=0.9, colsample_bynode=None,\n              colsample_bytree=0.6, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=[&#x27;auc&#x27;, &#x27;aucpr&#x27;, &#x27;logloss&#x27;],\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=300, n_jobs=8,\n              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=0.9, colsample_bynode=None,\n              colsample_bytree=0.6, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=[&#x27;auc&#x27;, &#x27;aucpr&#x27;, &#x27;logloss&#x27;],\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=300, n_jobs=8,\n              num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"y_pred_proba = classifier.predict_proba(embedobj.embeddings[300000:])","metadata":{"_uuid":"821da462-507a-4f53-a2a3-86fa754c7dd4","_cell_guid":"0dd2455e-9304-4529-8a74-69d86aa59655","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:22:58.289798Z","iopub.execute_input":"2025-03-16T17:22:58.290108Z","iopub.status.idle":"2025-03-16T17:22:58.295566Z","shell.execute_reply.started":"2025-03-16T17:22:58.290085Z","shell.execute_reply":"2025-03-16T17:22:58.294759Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"preds=y_pred_proba[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:22:59.321678Z","iopub.execute_input":"2025-03-16T17:22:59.321964Z","iopub.status.idle":"2025-03-16T17:22:59.325725Z","shell.execute_reply.started":"2025-03-16T17:22:59.321943Z","shell.execute_reply":"2025-03-16T17:22:59.324996Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"preds.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:22:59.529062Z","iopub.execute_input":"2025-03-16T17:22:59.529351Z","iopub.status.idle":"2025-03-16T17:22:59.533698Z","shell.execute_reply.started":"2025-03-16T17:22:59.529328Z","shell.execute_reply":"2025-03-16T17:22:59.533093Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(500,)"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"## Evaluation","metadata":{"_uuid":"ff269d4b-a9aa-43b3-bd71-0b92dfed6065","_cell_guid":"d61ce32a-088c-40a6-9f1c-3839916ec1d0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Assume y_proba[:, 1] contains probabilities for class 1\ny_pred = (preds >= 0.5).astype(int)  # Convert to binary labels (0 or 1)\n\n# Compute confusion matrix\ncm = confusion_matrix(y_train[300000:], y_pred)\n\nprint(cm)","metadata":{"_uuid":"7f796fc2-b787-401a-be9a-d76edf30f0b6","_cell_guid":"769304d4-d49c-40f7-b232-8c92cf0ef99e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-16T17:23:10.927773Z","iopub.execute_input":"2025-03-16T17:23:10.928076Z","iopub.status.idle":"2025-03-16T17:23:10.936372Z","shell.execute_reply.started":"2025-03-16T17:23:10.928054Z","shell.execute_reply":"2025-03-16T17:23:10.935508Z"}},"outputs":[{"name":"stdout","text":"[[499   0]\n [  0   1]]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}